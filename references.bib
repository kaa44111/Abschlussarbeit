
@misc{noauthor_torchvision_nodate,
	title = {torchvision — {Torchvision} 0.20 documentation},
	url = {https://pytorch.org/vision/stable/index.html},
	urldate = {2024-05-28},
	journal = {Pytorch Documentation: torchvision},
}

@inproceedings{dodge_study_2017,
	title = {A {Study} and {Comparison} of {Human} and {Deep} {Learning} {Recognition} {Performance} under {Visual} {Distortions}},
	url = {https://ieeexplore.ieee.org/abstract/document/8038465},
	doi = {10.1109/ICCCN.2017.8038465},
	abstract = {Deep neural networks (DNNs) achieve excellent performance on standard classification tasks. However, under image quality distortions such as blur and noise, classification accuracy becomes poor. In this work, we compare the performance of DNNs with human subjects on distorted images. We show that, although DNNs perform better than or on par with humans on good quality images, DNN performance is still much lower than human performance on distorted images. We additionally find that there is little correlation in errors between DNNs and human subjects. This could be an indication that the internal representation of images are different between DNNs and the human visual system. These comparisons with human performance could be used to guide future development of more robust DNNs.},
	urldate = {2024-10-27},
	booktitle = {2017 26th {International} {Conference} on {Computer} {Communication} and {Networks} ({ICCCN})},
	author = {Dodge, Samuel and Karam, Lina},
	month = jul,
	year = {2017},
	keywords = {Distortion, Neural networks, Robustness, Standards, Testing, Training, Visual systems},
	pages = {1--7},
}

@book{ruckert_methods_2023,
	title = {Methods and datasets for segmentation of minimally invasive surgical instruments in endoscopic images and videos: {A} review of the state of the art},
	shorttitle = {Methods and datasets for segmentation of minimally invasive surgical instruments in endoscopic images and videos},
	abstract = {In the field of computer- and robot-assisted minimally invasive surgery, enormous progress has been made in recent years based on the recognition of surgical instruments in endoscopic images. Especially the determination of the position and type of the instruments is of great interest here. Current work involves both spatial and temporal information with the idea, that the prediction of movement of surgical tools over time may improve the quality of final segmentations. The provision of publicly available datasets has recently encouraged the development of new methods, mainly based on deep learning. In this review, we identify datasets used for method development and evaluation, as well as quantify their frequency of use in the literature. We further present an overview of the current state of research regarding the segmentation and tracking of minimally invasive surgical instruments in endoscopic images. The paper focuses on methods that work purely visually without attached markers of any kind on the instruments, taking into account both single-frame segmentation approaches as well as those involving temporal information. A discussion of the reviewed literature is provided, highlighting existing shortcomings and emphasizing available potential for future developments. The publications considered were identified through the platforms Google Scholar, Web of Science, and PubMed. The search terms used were "instrument segmentation", "instrument tracking", "surgical tool segmentation", and "surgical tool tracking" and result in 408 articles published between 2015 and 2022 from which 109 were included using systematic selection criteria.},
	author = {Rückert, Tobias and Rueckert, Daniel and Palm, Christoph},
	month = apr,
	year = {2023},
	doi = {10.48550/arXiv.2304.13014},
	keywords = {nicht gelesen},
}

@phdthesis{renner_deep_2023,
	title = {Deep {Learning} basierte {Segmentierung} und {Transformation} von {Spike}-{Rauschen} in der {MR}- {Bildgebung}},
	author = {Renner, Philipp},
	year = {2023},
	keywords = {gelesen, relevant},
}

@book{jahne_digitale_2024,
	address = {Berlin, Heidelberg},
	title = {Digitale {Bildverarbeitung}: und {Bildgewinnung}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-662-59509-1 978-3-662-59510-7},
	shorttitle = {Digitale {Bildverarbeitung}},
	url = {https://link.springer.com/10.1007/978-3-662-59510-7},
	language = {de},
	urldate = {2024-09-24},
	publisher = {Springer},
	author = {Jähne, Bernd},
	year = {2024},
	doi = {10.1007/978-3-662-59510-7},
	keywords = {Bildanalyse, Bildaufnahme, Bildverarbeitungsoperation, Klassifikation, Klassifizierung, Maschinelles Sehen, Maschinensehen, Merkmalsextraktion, Messtechnik, gelesen, künstliche Intelligenz, reviewed},
}

@book{braga-neto_fundamentals_2024,
	address = {Cham},
	title = {Fundamentals of {Pattern} {Recognition} and {Machine} {Learning}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-031-60949-7 978-3-031-60950-3},
	url = {https://link.springer.com/10.1007/978-3-031-60950-3},
	language = {en},
	urldate = {2024-10-18},
	publisher = {Springer International Publishing},
	author = {Braga-Neto, Ulisses},
	year = {2024},
	doi = {10.1007/978-3-031-60950-3},
	keywords = {Bioinformatics, Bootstrap, Clustering, Cross-Validation, Decision Trees, Dimensionality Reduction, Error Estimation, Feature Selection, Gaussian Mixture Modeling, Gaussian Process, K-means Clustering, Machine Learning, Materials Informatics, Multidimensional Scaling, Neural Networks, Pattern Recognition, Principal Component Analysis, Regression, Support Vector Machines, Vapnik-Chervonenkis Theory},
}

@book{szeliski_computer_2022,
	address = {Cham},
	series = {Texts in {Computer} {Science}},
	title = {Computer {Vision}: {Algorithms} and {Applications}},
	copyright = {https://www.springer.com/tdm},
	isbn = {978-3-030-34371-2 978-3-030-34372-9},
	shorttitle = {Computer {Vision}},
	url = {https://link.springer.com/10.1007/978-3-030-34372-9},
	language = {en},
	urldate = {2024-08-06},
	publisher = {Springer International Publishing},
	author = {Szeliski, Richard},
	year = {2022},
	doi = {10.1007/978-3-030-34372-9},
	keywords = {3D Reconstruction, Computational Photography, Computer Vision, Deep Learning, Feature Detection and Matching, Image Processing, Image Segmentation, Image Stitching, Image-Based Rendering, Motion Estimation, Scene Recognition, Structure from Motion, gelesen, reviewed, sehr hoch!},
}

@book{beyerer_automatische_2024,
	address = {Berlin, Heidelberg},
	title = {Automatische {Sichtprüfung}: {Grundlagen}, {Methoden} und {Praxis} der {Bildgewinnung} und {Bildauswertung}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-662-69950-8 978-3-662-69951-5},
	shorttitle = {Automatische {Sichtprüfung}},
	url = {https://link.springer.com/10.1007/978-3-662-69951-5},
	language = {de},
	urldate = {2024-10-18},
	publisher = {Springer},
	author = {Beyerer, Jürgen and Puente León, Fernando and Frese, Christian and Meyer, Johannes},
	year = {2024},
	doi = {10.1007/978-3-662-69951-5},
	keywords = {Automated Visual, Automatische Inspektion, Automatische Sichtprüfung, Bildauswertung, Bildverarbeitung, Industrial Machine Vision, Inspection, Optische Inspektion, Optische Messtechnik, Optische Qualitätsprüfung, sehr hoch!},
}

@book{jo_deep_2023,
	address = {Cham},
	title = {Deep {Learning} {Foundations}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-031-32878-7 978-3-031-32879-4},
	url = {https://link.springer.com/10.1007/978-3-031-32879-4},
	language = {en},
	urldate = {2024-10-01},
	publisher = {Springer International Publishing},
	author = {Jo, Taeho},
	year = {2023},
	doi = {10.1007/978-3-031-32879-4},
	keywords = {Deep K nearest Neighbor, Deep Learning, Deep Naïve Bayes, Deep Support Vector Machine, Multiple Layer Perceptron, nicht gelesen, sehr hoch!},
}

@mastersthesis{restat_automatisierte_2021,
	title = {Automatisierte {Fehlererkennung} einer {Microservice}-{Anwendung} basierend auf {Log}-{Dateien}},
	author = {Restat, Valerie},
	year = {2021},
}

@phdthesis{schmitz_machine_2021,
	type = {Doktor},
	title = {Machine {Learning} in {Industrial} {Applications} {Insights} {Gained} from {Selected} {Studies}},
	author = {Schmitz, Markus},
	year = {2021},
}

@phdthesis{baghdadi_optimierung_2023,
	type = {B.{Sc}},
	title = {Optimierung der {Produktionsprozesse} mittels {KI} in {Industrie} 5.0},
	author = {Baghdadi, Fariborz},
	year = {2023},
}

@article{noauthor_detektion_nodate,
	title = {Detektion von {Konstruktionsfehlern} durch eine automatisierte {Objekterkennung} mittels {Deep} {Learning}},
	url = {https://www.researchgate.net/publication/346628303_Detektion_von_Konstruktionsfehlern_durch_eine_automatisierte_Objekterkennung_mittels_Deep_Learning},
	urldate = {2024-06-07},
	keywords = {relevant},
}

@phdthesis{vinogradova_explainable_2023,
	type = {Doktor},
	title = {Explainable {Artificial} {Intelligence} for {Image} {Segmentation} and for {Estimation} of {Optical} {Aberrations}},
	author = {Vinogradova, Kira},
	year = {2023},
}

@phdthesis{muller_industrielle_2020,
	address = {M.Sc},
	title = {Industrielle {Bildverarbeitung} zur {Qualitätskontrolle} in {Produktionslinien}: {Entwicklung} einer {Entscheidungslogik} zur {Anwendungsfallspezifischen} {Auswahl} von {Hard}- und {Software}},
	language = {de},
	author = {Müller, Michael},
	year = {2020},
}

@mastersthesis{heidari_classifying_2019,
	title = {Classifying {Material} {Defects} with {Convolutional} {Neural} {Networks} and {Image} {Processing}},
	url = {https://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-387797},
	abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
	language = {eng},
	urldate = {2024-07-18},
	author = {Heidari, Jawid},
	year = {2019},
}

@mastersthesis{voithofer_verwendung_2019,
	title = {Verwendung von {Deep} {Learning} für die {Erkennung} von {Werbung} am {Straßenrand} in {Dashcam}-{Videos}},
	language = {en},
	author = {Voithofer, Mario},
	year = {2019},
}

@phdthesis{sauer_objekterkennung_2020,
	type = {B.{Sc}},
	title = {Objekterkennung mit {Hintergrundsubtraktion}},
	language = {de},
	author = {Sauer, Dr Tomas},
	year = {2020},
}

@phdthesis{lin_cloud_2022,
	address = {Dresden},
	type = {Doktor},
	title = {Cloud simulation based bridge damage identification enhanced by computer vision and augmented reality},
	language = {en},
	school = {Institut fur Bauinformatik, Fakultät Bauingenieurwesen, TU Dresden},
	author = {Lin, Fangzheng},
	collaborator = {{Technische Universität Dresden}},
	year = {2022},
}

@mastersthesis{czarnetzki_konzeption_2021,
	title = {Konzeption und {Entwicklung} eines {Deep} {Learning} basierten {Embedded} {Vision} {Systems} zur {Analyse} von {Laufwegen}},
	abstract = {This thesis describes the development, conception and realization of an embedded vision system. The system task is to analyse the walking path of persons. A development board designed for the use of machine learning applications forms the basis of the system. The walking path analysis is achieved by a Deep Learning based person detection performed by the neural network YOLOv3-Tiny. Based on the data acquired by the person detection it is determined how often certain sub-areas of a monitored area are used by people as walking paths. A corresponding visualization is generated to illustrate the results.},
	language = {de},
	author = {Czarnetzki, Christopher},
	year = {2021},
}

@phdthesis{mitschke_konvolutionare_2022,
	title = {Konvolutionäre neuronale {Netze} in der industriellen {Bildverarbeitung} und {Robotik}},
	url = {https://library.oapen.org/handle/20.500.12657/58037},
	abstract = {In the first part of this dissertation, a framework for the design of a CNN for FPGAs is presented, consisting of a preprocessing algorithm, an augmentation technique, a custom quantization scheme and a pruning step of the CNN. The combination of conventional image processing with neural networks is shown in the second part by an example from robotics, where an image-based visual servoing process is successfully conducted for a gripping process of a robot.},
	language = {German},
	urldate = {2024-08-21},
	school = {KIT Scientific Publishing},
	author = {Mitschke, Norbert},
	year = {2022},
	doi = {10.5445/KSP/1000146397},
	note = {Accepted: 2022-08-22T09:17:26Z},
	keywords = {Agriculture, Bildverarbeitung, CNN, Engineering, FPGA, Industrial processes::TH Energy technology and engineering::THR Electrical engineering, bildbasierte Regelung, gelesen, image based visual servoing, künstliche neuronale Netze, relevant, thema EDItEUR::T Technology},
}

@mastersthesis{tabares_machine_2020,
	title = {Machine {Learning} {Image} {Segmentation} to {Improve} {Object} {Recognition} in {Mixed} {Reality}},
	author = {Tabares, Guillermo Fernando Esquivel},
	year = {2020},
}

@phdthesis{xiao_automated_2022,
	title = {{AUTOMATED} {VISUAL} {DEFECT} {DETECTION} {USING} {DEEP} {LEARNING}},
	language = {en},
	author = {Xiao, Loh},
	year = {2022},
}

@phdthesis{kaya_aspekte_2022,
	title = {Aspekte der {Datensammlung} beim {Einsatz} von {Convolutional} {Neural} {Network} für das autonome {Fahren}},
	author = {Kaya, Arda},
	year = {2022},
}

@phdthesis{hertrich_cnn-basierte_2024,
	title = {{CNN}-basierte semantische {Segmentierung} von {Gebäudetypen} mittels hochaufgelöster {Luftbilder} und normalisierten digitalen {Oberflächenmodellen}},
	author = {Hertrich, Moritz},
	year = {2024},
	keywords = {relevant},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	isbn = {978-0-262-03561-3},
	url = {https://www.deeplearningbook.org},
	abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
	language = {en},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	month = nov,
	year = {2016},
	note = {Google-Books-ID: Np9SDQAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General, Computers / Computer Science, Computers / Data Science / Machine Learning, nicht gelesen, sehr hoch!},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2024-08-12},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Mathematics and computing, nicht gelesen, sehr hoch!},
	pages = {436--444},
}

@book{zhang_dive_2024,
	address = {Cambridge New York Port Melbourne New Delhi Singapore},
	title = {Dive into deep learning},
	isbn = {978-1-00-938943-3},
	language = {eng},
	publisher = {Cambridge University Press},
	author = {Zhang, Aston and Lipton, Zachary and Li, Mu and Smola, Alexander J.},
	year = {2024},
	keywords = {nicht gelesen, sehr hoch!},
}

@book{demant_industrielle_2011,
	address = {Berlin, Heidelberg},
	title = {Industrielle {Bildverarbeitung}: {Wie} optische {Qualitätskontrolle} wirklich funktioniert},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-642-13096-0 978-3-642-13097-7},
	shorttitle = {Industrielle {Bildverarbeitung}},
	url = {https://link.springer.com/10.1007/978-3-642-13097-7},
	language = {de},
	urldate = {2024-08-01},
	publisher = {Springer},
	author = {Demant, Christian and Streicher-Abel, Bernd and Springhoff, Axel},
	year = {2011},
	doi = {10.1007/978-3-642-13097-7},
	keywords = {gelesen, quality control, reliability, safety and risk},
}

@book{suse_bildverarbeitung_2014,
	address = {Wiesbaden},
	title = {Bildverarbeitung und {Objekterkennung}: {Computer} {Vision} in {Industrie} und {Medizin}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-8348-2605-3 978-3-8348-2606-0},
	shorttitle = {Bildverarbeitung und {Objekterkennung}},
	url = {https://link.springer.com/10.1007/978-3-8348-2606-0},
	language = {de},
	urldate = {2024-08-01},
	publisher = {Springer Fachmedien},
	author = {Süße, Herbert and Rodner, Erik},
	year = {2014},
	doi = {10.1007/978-3-8348-2606-0},
	keywords = {Abtasttheoreme, Ausgleichsrechnung, Automatisierung, Automatisierungstechnik, Bildsegmentierung, Bildtransformationen, E-Technik, Elektronik, Fertigung, Geometrie der Abbildungsprozesse, Informationstechnik, Korrelation, Lineare Algebra, Maschinelles Lesen, Maschinelles Sehen, Medizintechnik, Momente, Matching, Merkmale, Objekterkennung, Operationen Falltung, Optik, Orts-Frequenz-Darstellung, Qualitätssicherung, Robotik, Stochastik, Stochastische Bildsignale, Technik, Textturen, gelesen, reviewed},
}

@article{liu_deep_2023,
	title = {Deep learning in image segmentation for mineral production: {A} review},
	volume = {180},
	issn = {0098-3004},
	shorttitle = {Deep learning in image segmentation for mineral production},
	url = {https://www.sciencedirect.com/science/article/pii/S0098300423001590},
	doi = {10.1016/j.cageo.2023.105455},
	abstract = {Mineral image segmentation is widely used in mining, sorting, exploration, composition analysis, and other production works. The burgeoning field of deep learning provides preferred solutions for mineral image segmentation. We present a review of recent literature in this direction, covering the module components, encoder-decoders architecture, representative networks, mineral image datasets, performance metrics, and state-of-the-art models. In the application performance survey, the review contents include mineral type, image type, image resolution, image data quantity, architecture selection, and encoder network construction, as well as summarizes the advantages of deep learning-based mineral image segmentation methods. We conducted small-scale experiments for the current mainstream architectures and visualize the segmentation results for performance comparison. We also investigated the application challenges and bottlenecks of deep learning-based methods, propose several innovative directions, and discuss promising future applications.},
	urldate = {2024-10-14},
	journal = {Computers \& Geosciences},
	author = {Liu, Yang and Wang, Xueyi and Zhang, Zelin and Deng, Fang},
	month = nov,
	year = {2023},
	keywords = {Application performance survey, Deep learning, Encoder-decoders architecture, Intelligent mineral industry, Mineral image segmentation},
	pages = {105455},
}

@article{zhang_modified_2023,
	title = {Modified {U}-{Net} for plant diseased leaf image segmentation},
	volume = {204},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169922008195},
	doi = {10.1016/j.compag.2022.107511},
	abstract = {Early detection and recognition of plant disease is a prerequisite for controlling plant disease, and one of the key steps is to segment plant diseased leaf images. However, this task is challenging because diseased leaf images are often very complex, with irregular shapes, variable sizes, various shapes, rich colors, fuzzy boundaries and messy backgrounds. An improved U-Net (MU-Net) is constructed for plant diseased leaf image segmentation by introducing a residual block (Resblock) and a residual path (Respath). Resblock is introduced into U-Net to overcome gradient disappearance and explosion problems, and 2 Respaths are used instead of 2 skip connections to improve the transformation of corresponding feature information between the contraction path and the expansion path. Furthermore, Resblock and Respath are combined, which can increase the network depth and improve the network’s expression ability. Experimental results on a plant diseased leaf image dataset show that the proposed method can improve the accuracy and efficiency of plant diseased leaf image segmentation.},
	urldate = {2024-08-19},
	journal = {Computers and Electronics in Agriculture},
	author = {Zhang, Shanwen and Zhang, Chuanlei},
	month = jan,
	year = {2023},
	keywords = {Modified U-Net (MU-Net), Plant diseased leaf image segmentation, Residual block (Resblock), Residual path (Respath), sehr hoch!},
	pages = {107511},
}

@article{eversberg_combining_2024,
	title = {Combining {Synthetic} {Images} and {Deep} {Active} {Learning}: {Data}-{Efficient} {Training} of an {Industrial} {Object} {Detection} {Model}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2313-433X},
	shorttitle = {Combining {Synthetic} {Images} and {Deep} {Active} {Learning}},
	url = {https://www.mdpi.com/2313-433X/10/1/16},
	doi = {10.3390/jimaging10010016},
	abstract = {Generating synthetic data is a promising solution to the challenge of limited training data for industrial deep learning applications. However, training on synthetic data and testing on real-world data creates a sim-to-real domain gap. Research has shown that the combination of synthetic and real images leads to better results than those that are generated using only one source of data. In this work, the generation of synthetic training images via physics-based rendering is combined with deep active learning for an industrial object detection task to iteratively improve model performance over time. Our experimental results show that synthetic images improve model performance, especially at the beginning of the model’s life cycle with limited training data. Furthermore, our implemented hybrid query strategy selects diverse and informative new training images in each active learning cycle, which outperforms random sampling. In conclusion, this work presents a workflow to train and iteratively improve object detection models with a small number of real-world images, leading to data-efficient and cost-effective computer vision models.},
	language = {en},
	number = {1},
	urldate = {2024-10-14},
	journal = {Journal of Imaging},
	author = {Eversberg, Leon and Lambrecht, Jens},
	month = jan,
	year = {2024},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {active learning, computer vision, data efficiency, deep active learning, deep learning, image synthesis, industrial application, object detection, synthetic images, turbine blade},
	pages = {16},
}

@inproceedings{moosavian_improved_2024,
	title = {An {Improved} {U}-{Net} {Image} {Segmentation} {Network} for {Crankshaft} {Surface} {Defect} {Detection}},
	url = {https://ieeexplore.ieee.org/abstract/document/10491179},
	doi = {10.1109/MVIP62238.2024.10491179},
	abstract = {Crankshaft is one of the mechanical components of the vehicle engine, and quality control of it holds significant importance in the production line. In this paper, a vision-based system was developed to detect apparent structural defects on the crankshaft surface. By examining the different approaches in computer vision tasks, the semantic segmentation technique was chosen to solve this problem. In the first stage, a dataset consisting of 400 crankshaft experimental images with structural defects such as scratch, pitting, and grinding were collected. Then, the Convolutional Neural Network (CNN) with MobileNet architecture was trained to detect apparent defects, and an Intersection over Union (IoU) evaluation criteria of 64.7\% was obtained. In the third stage, some image processing techniques were used to increase the performance. By applying the DexiNed edge detection filter on the train-set images, the IoU was increased by 8.4\%. Considering the importance of this issue in the automotive industry, it has been tried again to boost the performance by augmenting the dataset images. On the other hand, this can also prevent overfitting of the model. By training the model under the same conditions as the previous stages, the IoU in this stage increased by 13.2\% and reached 86.3\%.},
	urldate = {2024-10-14},
	booktitle = {2024 13th {Iranian}/3rd {International} {Machine} {Vision} and {Image} {Processing} {Conference} ({MVIP})},
	author = {Moosavian, Ashkan and Bagheri, Elmira and Yazdanijoo, Alireza and Barshooi, Amir Hossein},
	month = mar,
	year = {2024},
	note = {ISSN: 2166-6784},
	keywords = {Computer architecture, Data augmentation, Image edge detection, Machine vision, Supervised learning, Training, Transfer learning, crankshaft, deep learning, defect detection, pitting, scratch, segmentation},
	pages = {1--6},
}

@article{xin_autonomous_2024,
	title = {Autonomous detection of steel corrosion spatial variability in reinforced concrete using {X}-ray technology and deep learning-based semantic segmentation},
	volume = {158},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580523005125},
	doi = {10.1016/j.autcon.2023.105252},
	abstract = {Correctly determining the spatial distribution of steel corrosion within a structural member is critical for estimating the remaining service life of deteriorating reinforced concrete (RC) structures. While X-ray technology serves as a nondestructive inspection method, existing challenges persist, particularly in semi-automated corrosion boundary detection. This paper describes a deep learning-based semantic segmentation framework to autonomously detect X-ray images associated with RC, facilitating the visualization of nonuniform steel corrosion distribution. X-ray images were collected from a comprehensive experiment using RC specimens with various structural details by two accelerated corrosion methods. Four deep learning models were constructed, trained, and compared based on the database containing the original X-ray images and the corresponding pixel-level labels. The results demonstrate that the proposed autonomous detection method can segment uncorroded steel at a very high level of global accuracy without time-consuming work, outperforming traditional methods in terms of both accuracy and efficiency.},
	urldate = {2024-10-14},
	journal = {Automation in Construction},
	author = {Xin, Jiyu and Akiyama, Mitsuyoshi and Frangopol, Dan M.},
	month = feb,
	year = {2024},
	keywords = {Autonomous detection, Computer vision, Deep learning, Reinforced concrete, Semantic segmentation, Spatial variability, Steel corrosion, X-ray},
	pages = {105252},
}

@article{zhao_interaction_2024,
	title = {Interaction semantic segmentation network via progressive supervised learning},
	volume = {35},
	issn = {1432-1769},
	url = {https://doi.org/10.1007/s00138-023-01500-4},
	doi = {10.1007/s00138-023-01500-4},
	abstract = {Semantic segmentation requires both low-level details and high-level semantics, without losing too much detail and ensuring the speed of inference. Most existing segmentation approaches leverage low- and high-level features from pre-trained models. We propose an interaction semantic segmentation network via Progressive Supervised Learning (ISSNet). Unlike a simple fusion of two sets of features, we introduce an information interaction module to embed semantics into image details, they jointly guide the response of features in an interactive way. We develop a simple yet effective boundary refinement module to provide refined boundary features for matching corresponding semantic. We introduce a progressive supervised learning strategy throughout the training level to significantly promote network performance, not architecture level. Our proposed ISSNet shows optimal inference time. We perform extensive experiments on four datasets, including Cityscapes, HazeCityscapes, RainCityscapes and CamVid. In addition to performing better in fine weather, proposed ISSNet also performs well on rainy and foggy days. We also conduct ablation study to demonstrate the role of our proposed component. Code is available at: https://github.com/Ruini94/ISSNet},
	language = {en},
	number = {2},
	urldate = {2024-10-14},
	journal = {Machine Vision and Applications},
	author = {Zhao, Ruini and Xie, Meilin and Feng, Xubin and Guo, Min and Su, Xiuqin and Zhang, Ping},
	month = feb,
	year = {2024},
	keywords = {Boundary refinement, Information interaction, Progressive supervised learning, Semantic segmentation},
	pages = {26},
}

@inproceedings{long_fully_2015,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html},
	urldate = {2024-10-14},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	year = {2015},
	pages = {3431--3440},
}

@article{archana_deep_2024,
	title = {Deep learning models for digital image processing: a review},
	volume = {57},
	issn = {1573-7462},
	shorttitle = {Deep learning models for digital image processing},
	url = {https://doi.org/10.1007/s10462-023-10631-z},
	doi = {10.1007/s10462-023-10631-z},
	abstract = {Within the domain of image processing, a wide array of methodologies is dedicated to tasks including denoising, enhancement, segmentation, feature extraction, and classification. These techniques collectively address the challenges and opportunities posed by different aspects of image analysis and manipulation, enabling applications across various fields. Each of these methodologies contributes to refining our understanding of images, extracting essential information, and making informed decisions based on visual data. Traditional image processing methods and Deep Learning (DL) models represent two distinct approaches to tackling image analysis tasks. Traditional methods often rely on handcrafted algorithms and heuristics, involving a series of predefined steps to process images. DL models learn feature representations directly from data, allowing them to automatically extract intricate features that traditional methods might miss. In denoising, techniques like Self2Self NN, Denoising CNNs, DFT-Net, and MPR-CNN stand out, offering reduced noise while grappling with challenges of data augmentation and parameter tuning. Image enhancement, facilitated by approaches such as R2R and LE-net, showcases potential for refining visual quality, though complexities in real-world scenes and authenticity persist. Segmentation techniques, including PSPNet and Mask-RCNN, exhibit precision in object isolation, while handling complexities like overlapping objects and robustness concerns. For feature extraction, methods like CNN and HLF-DIP showcase the role of automated recognition in uncovering image attributes, with trade-offs in interpretability and complexity. Classification techniques span from Residual Networks to CNN-LSTM, spotlighting their potential in precise categorization despite challenges in computational demands and interpretability. This review offers a comprehensive understanding of the strengths and limitations across methodologies, paving the way for informed decisions in practical applications. As the field evolves, addressing challenges like computational resources and robustness remains pivotal in maximizing the potential of image processing techniques.},
	language = {en},
	number = {1},
	urldate = {2024-08-12},
	journal = {Artificial Intelligence Review},
	author = {Archana, R. and Jeevaraj, P. S. Eliahim},
	month = jan,
	year = {2024},
	keywords = {2024 Review, Convolutional neural networks (CNN), Deep learning models, Image processing, mittel, relevant},
	pages = {11},
}

@incollection{walke_image_2023,
	title = {Image {Processing} in {Industrial} {Chemical} {Engineering} {Trends} and {Applications}},
	isbn = {978-1-66848-618-4},
	abstract = {This chapter gives a thorough overview of image processing's uses and potential in industrial chemical engineering. Image processing can provide precise and in-depth information about chemical processes, products, and its significance in this field is highlighted. The foundations of image processing are covered in this chapter, including image formation and acquisition, image preprocessing, feature extraction, and selection. The applications of image-based process monitoring and control, image analysis for product quality control, and the newest developments and difficulties in machine learning in image-based chemical engineering are also covered. The section on machine learning in image-based chemical engineering gives a general overview of machine learning methods and how they are used in the field of chemical engineering. The chapter's discussion of image processing's limitations in chemical engineering, as well as current trends and future research prospects, come to close.},
	author = {Walke, Santosh and Mandke, Manoj and Tapre, Ravi and Naniwadekar, Makarand and Thakar, Chetan and Jadhav, Sandhya},
	month = jun,
	year = {2023},
	doi = {10.4018/978-1-6684-8618-4.ch021},
	keywords = {relevant},
	pages = {348--363},
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {https://www.semanticscholar.org/paper/995c5f5e62614fcb4d2796ad2faab969da51713e},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
	urldate = {2024-10-14},
	journal = {ArXiv},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
}

@article{wang_understanding_2018,
	title = {Understanding {Convolution} for {Semantic} {Segmentation}},
	url = {https://ieeexplore.ieee.org/document/8354267/},
	doi = {10.1109/WACV.2018.00163},
	abstract = {Recent advances in deep learning, especially deep convolutional neural networks (CNNs), have led to significant improvement over previous semantic segmentation systems. Here we show how to improve pixel-wise semantic segmentation by manipulating convolution-related operations that are of both theoretical and practical value. First, we design dense upsampling convolution (DUC) to generate pixel-level prediction, which is able to capture and decode more detailed information that is generally missing in bilinear upsampling. Second, we propose a hybrid dilated convolution (HDC) framework in the encoding phase. This framework 1) effectively enlarges the receptive fields (RF) of the network to aggregate global information; 2) alleviates what we call the "gridding issue"caused by the standard dilated convolution operation. We evaluate our approaches thoroughly on the Cityscapes dataset, and achieve a state-of-art result of 80.1\% mIOU in the test set at the time of submission. We also have achieved state-of-theart overall on the KITTI road estimation benchmark and the PASCAL VOC2012 segmentation task. Our source code can be found at https://github.com/TuSimple/TuSimple-DUC.},
	urldate = {2024-10-14},
	journal = {2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
	author = {Wang, Panqu and Chen, Pengfei and Yuan, Ye and Liu, Ding and Huang, Zehua and Hou, Xiaodi and Cottrell, Garrison},
	month = mar,
	year = {2018},
	note = {Conference Name: 2018 IEEE Winter Conference on Applications of Computer Vision (WACV)
ISBN: 9781538648865
Place: Lake Tahoe, NV
Publisher: IEEE},
	pages = {1451--1460},
}

@book{ketkar_deep_2021,
	address = {Berkeley, CA},
	title = {Deep {Learning} with {Python}: {Learn} {Best} {Practices} of {Deep} {Learning} {Models} with {PyTorch}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4842-5363-2 978-1-4842-5364-9},
	shorttitle = {Deep {Learning} with {Python}},
	url = {http://link.springer.com/10.1007/978-1-4842-5364-9},
	language = {en},
	urldate = {2024-08-20},
	publisher = {Apress},
	author = {Ketkar, Nikhil and Moolayil, Jojo},
	year = {2021},
	doi = {10.1007/978-1-4842-5364-9},
	keywords = {Advanced PyTorch, Deep Learning, Deep Networks, Machine Learning, PyTorch, Python, nicht gelesen},
}

@misc{noauthor_explain_2024,
	title = {Explain {Image} {Segmentation} : {Techniques} and {Applications}},
	shorttitle = {Explain {Image} {Segmentation}},
	url = {https://www.geeksforgeeks.org/explain-image-segmentation-techniques-and-applications/},
	abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
	language = {en-US},
	urldate = {2024-08-12},
	journal = {GeeksforGeeks},
	month = may,
	year = {2024},
	note = {Section: AI-ML-DS},
	keywords = {nicht gelesen},
}

@article{abdolrasol_artificial_2021,
	title = {Artificial {Neural} {Networks} {Based} {Optimization} {Techniques}: {A} {Review}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	shorttitle = {Artificial {Neural} {Networks} {Based} {Optimization} {Techniques}},
	url = {https://www.mdpi.com/2079-9292/10/21/2689},
	doi = {10.3390/electronics10212689},
	abstract = {In the last few years, intensive research has been done to enhance artificial intelligence (AI) using optimization techniques. In this paper, we present an extensive review of artificial neural networks (ANNs) based optimization algorithm techniques with some of the famous optimization techniques, e.g., genetic algorithm (GA), particle swarm optimization (PSO), artificial bee colony (ABC), and backtracking search algorithm (BSA) and some modern developed techniques, e.g., the lightning search algorithm (LSA) and whale optimization algorithm (WOA), and many more. The entire set of such techniques is classified as algorithms based on a population where the initial population is randomly created. Input parameters are initialized within the specified range, and they can provide optimal solutions. This paper emphasizes enhancing the neural network via optimization algorithms by manipulating its tuned parameters or training parameters to obtain the best structure network pattern to dissolve the problems in the best way. This paper includes some results for improving the ANN performance by PSO, GA, ABC, and BSA optimization techniques, respectively, to search for optimal parameters, e.g., the number of neurons in the hidden layers and learning rate. The obtained neural net is used for solving energy management problems in the virtual power plant system.},
	language = {en},
	number = {21},
	urldate = {2024-10-13},
	journal = {Electronics},
	author = {Abdolrasol, Maher G. M. and Hussain, S. M. Suhail and Ustun, Taha Selim and Sarker, Mahidur R. and Hannan, Mahammad A. and Mohamed, Ramizi and Ali, Jamal Abd and Mekhilef, Saad and Milad, Abdalrhman},
	month = jan,
	year = {2021},
	note = {Number: 21
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {ABC, ANN enhancement, BSA, GA, PSO, artificial neural networks, machine learning, optimization algorithms},
	pages = {2689},
}

@article{kaur_systematic_2024,
	title = {A systematic review of object detection from images using deep learning},
	volume = {83},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-023-15981-y},
	doi = {10.1007/s11042-023-15981-y},
	abstract = {The development of object detection has led to huge improvements in human interaction systems. Object detection is a challenging task because it involves many parameters including variations in poses, resolution, occlusion, and daytime versus nighttime detection. This study surveys on various aspects of object detection that includes (1) basics of object detection, (2) object detection techniques, (3) datasets, (4) metrics and deep learning libraries. This study presents a systematic analysis of recent publications on object detection covering around 400 research articles and synthesised the findings to provide empirical answers to research questions. The review is based on relevant articles published from 2015 through 2022, as well as discussions of challenges and future directions in this field. Furthermore, the survey examined the contributions of various researchers concerning their respective application domains, while emphasizing the advantages and disadvantages of the research work. Despite the success of various methods proposed in literature for predicting results, there remains room for improvement in the accuracy of object detection.},
	language = {en},
	number = {4},
	urldate = {2024-10-04},
	journal = {Multimedia Tools and Applications},
	author = {Kaur, Jaskirat and Singh, Williamjeet},
	month = jan,
	year = {2024},
	keywords = {Artificial Intelligence, Backbone architecture, Computer vision, deep learning, object detection, object detection application, relevant},
	pages = {12253--12338},
}

@book{leontaris_inspection_2023,
	title = {{INSPECTION} {OF} {SURFACE} {DEFECTS} {IN} {METAL} {PROCESSING} {INDUSTRY} {USING} {UNET}-{BASED} {ARCHITECTURES}},
	author = {Leontaris, Lampros and Dimitriou, Nikolaos and Nikolousis, Apostolos and Tzovaras, Dimitrios and Papageorgiou, Elpiniki},
	month = jan,
	year = {2023},
	doi = {10.7712/150123.9870.444670},
	note = {Pages: 1233},
	keywords = {relevant},
}

@article{dong_investigation_2024,
	title = {Investigation of medical image segmentation techniques and analysis of key applications},
	issn = {2755-2721},
	url = {https://typeset.io/papers/investigation-of-medical-image-segmentation-techniques-and-56e4o2uujo},
	doi = {10.54254/2755-2721/32/20230179},
	abstract = {This research examines the application of the UNet convolutional neural network model, specifically for semantic segmentation tasks in the field of medical imaging, juxtaposing its efficacy with Fully Convolutional Networks (FCNs). The primary focus of this comparative analysis rests on the performance of the UNet model on the dataset employed for this study. Surpassing our initial expectations, the UNet model demonstrated remarkable performance superiority over the FCN model on the curated dataset, thereby suggesting its potential applicability and utility for analogous tasks within the realm of medical imaging. In a surprising turn of events, our trials revealed that data augmentation techniques did not usher in a notable enhancement in segmentation accuracy. This observation was especially striking given the substantial size of the dataset employed for the experiments, encompassing as many as 1000 images. This outcome suggests that the merits of data augmentation may not always come to the fore when dealing with considerably large datasets. This intriguing discovery prompts further exploration and investigation to uncover the underlying reasons behind this observed phenomenon. Moreover, it brings to light an open-ended research query - the quest for alternative methodologies that could potentially amplify segmentation accuracy when operating on large scale datasets in the sphere of medical imaging. As the field continues to evolve and mature, it is these open questions that will continue to push the boundaries of what is possible in medical image analysis.},
	language = {en},
	urldate = {2024-07-18},
	journal = {Applied and Computational Engineering},
	author = {Dong, Hao},
	month = jan,
	year = {2024},
}

@article{weimer_design_2016,
	title = {Design of deep convolutional neural network architectures for automated feature extraction in industrial inspection},
	volume = {65},
	issn = {0007-8506},
	url = {https://www.sciencedirect.com/science/article/pii/S0007850616300725},
	doi = {10.1016/j.cirp.2016.04.072},
	abstract = {Fast and reliable industrial inspection is a main challenge in manufacturing scenarios. However, the defect detection performance is heavily dependent on manually defined features for defect representation. In this contribution, we investigate a new paradigm from machine learning, namely deep machine learning by examining design configurations of deep Convolutional Neural Networks (CNN) and the impact of different hyper-parameter settings towards the accuracy of defect detection results. In contrast to manually designed image processing solutions, deep CNN automatically generate powerful features by hierarchical learning strategies from massive amounts of training data with a minimum of human interaction or expert process knowledge. An application of the proposed method demonstrates excellent defect detection results with low false alarm rates.},
	number = {1},
	urldate = {2024-09-29},
	journal = {CIRP Annals},
	author = {Weimer, Daniel and Scholz-Reiter, Bernd and Shpitalni, Moshe},
	month = jan,
	year = {2016},
	keywords = {Artificial intelligence, Deep machine learning, Quality assurance, sehr hoch!},
	pages = {417--420},
}

@inproceedings{jogin_feature_2018,
	title = {Feature {Extraction} using {Convolution} {Neural} {Networks} ({CNN}) and {Deep} {Learning}},
	url = {https://ieeexplore.ieee.org/abstract/document/9012507},
	doi = {10.1109/RTEICT42901.2018.9012507},
	abstract = {The Image classification is one of the preliminary processes, which humans learn as infants. The fundamentals of image classification lie in identifying basic shapes and geometry of objects around us. It is a process which involves the following tasks of pre-processing the image (normalization), image segmentation, extraction of key features and identification of the class. The current image classification techniques are much faster in run time and more accurate than ever before, they can be used for extensive applications including, security features, face recognition for authentication and authorization, traffic identification, medical diagnosis and other fields. The idea of image classification can be solved by different approaches. But the machine learning algorithms are the best among them. These algorithms are based on the idea proposed years ago, but couldn't be implemented due to lack of computational power. With the idea of deep learning, the models are trained better and are able to identify different levels of image representation. The convolutional neural networks revolutionized this field by learning the basic shapes in the first layers and evolving to learn features of the image in the deeper layers, resulting in more accurate image classification. The idea of Convolutional neural network was inspired by the hierarchical representation of neurons by Hubel and Wiesel in 1962, their work was based on the study of stimuli of the visual cortex in cat. It was a fundamental breakthrough in the field of computer vision in understanding the working of visual cortex in humans and animals. In this paper feature of an images is extracted using convolution neural network using the concept of deep learning. Further classification algorithms are implemented for various applications.},
	urldate = {2024-10-09},
	booktitle = {2018 3rd {IEEE} {International} {Conference} on {Recent} {Trends} in {Electronics}, {Information} \& {Communication} {Technology} ({RTEICT})},
	author = {Jogin, Manjunath and {Mohana} and Madhulika, M S and Divya, G D and Meghana, R K and Apoorva, S},
	month = may,
	year = {2018},
	keywords = {Activation Function Layer, Biological neural networks, Classification algorithms, Convolution Neural Network, Feature extraction, Fully Connected Layer, Image classification, Linear Classifier, Neurons, Pool Layer, Soft-max Classifier, Support vector machines, Training, kNN Classifier, sehr hoch!},
	pages = {2319--2323},
}

@article{yu_techniques_2023,
	title = {Techniques and {Challenges} of {Image} {Segmentation}: {A} {Review}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	shorttitle = {Techniques and {Challenges} of {Image} {Segmentation}},
	url = {https://www.mdpi.com/2079-9292/12/5/1199},
	doi = {10.3390/electronics12051199},
	abstract = {Image segmentation, which has become a research hotspot in the field of image processing and computer vision, refers to the process of dividing an image into meaningful and non-overlapping regions, and it is an essential step in natural scene understanding. Despite decades of effort and many achievements, there are still challenges in feature extraction and model design. In this paper, we review the advancement in image segmentation methods systematically. According to the segmentation principles and image data characteristics, three important stages of image segmentation are mainly reviewed, which are classic segmentation, collaborative segmentation, and semantic segmentation based on deep learning. We elaborate on the main algorithms and key techniques in each stage, compare, and summarize the advantages and defects of different segmentation models, and discuss their applicability. Finally, we analyze the main challenges and development trends of image segmentation techniques.},
	language = {en},
	number = {5},
	urldate = {2024-10-11},
	journal = {Electronics},
	author = {Yu, Ying and Wang, Chunping and Fu, Qiang and Kou, Renke and Huang, Fuyu and Yang, Boxiong and Yang, Tingting and Gao, Mingliang},
	month = jan,
	year = {2023},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {co-segmentation, deep learning, hoch, image processing, image segmentation, semantic segmentation},
	pages = {1199},
}

@misc{noauthor_klassifizierung_nodate,
	title = {Klassifizierung mit neuronalen {Netzen} {\textbar} {SpringerLink}},
	url = {https://link.springer.com/chapter/10.1007/978-3-658-28705-4_21},
	urldate = {2024-10-11},
}

@article{amjoud_object_2023,
	title = {Object {Detection} {Using} {Deep} {Learning}, {CNNs} and {Vision} {Transformers}: {A} {Review}},
	volume = {11},
	issn = {2169-3536},
	shorttitle = {Object {Detection} {Using} {Deep} {Learning}, {CNNs} and {Vision} {Transformers}},
	url = {https://ieeexplore.ieee.org/abstract/document/10098596},
	doi = {10.1109/ACCESS.2023.3266093},
	abstract = {Detecting objects remains one of computer vision and image understanding applications’ most fundamental and challenging aspects. Significant advances in object detection have been achieved through improved object representation and the use of deep neural network models. This paper examines more closely how object detection has evolved in the era of deep learning over the past years. We present a literature review on various state-of-the-art object detection algorithms and the underlying concepts behind these methods. We classify these methods into three main groups: anchor-based, anchor-free, and transformer-based detectors. Those approaches are distinct in the way they identify objects in the image. We discuss the insights behind these algorithms and experimental analyses to compare quality metrics, speed/accuracy tradeoffs, and training methodologies. The survey compares the major convolutional neural networks for object detection. It also covers the strengths and limitations of each object detector model and draws significant conclusions. We provide simple graphical illustrations summarising the development of object detection methods under deep learning. Finally, we identify where future research will be conducted.},
	urldate = {2024-10-11},
	journal = {IEEE Access},
	author = {Amjoud, Ayoub Benali and Amrouch, Mustapha},
	year = {2023},
	note = {Conference Name: IEEE Access},
	keywords = {Convolutional neural networks, Deep learning, Detectors, Feature extraction, Neural networks, Object detection, Transformers, Visualization, convolutional neural networks, deep learning, hoch, neural networks, review, survey, transformers},
	pages = {35479--35516},
}

@article{kaba_application_2023,
	title = {The {Application} of {Deep} {Learning} for the {Segmentation} and {Classification} of {Coronary} {Arteries}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-4418},
	url = {https://www.mdpi.com/2075-4418/13/13/2274},
	doi = {10.3390/diagnostics13132274},
	abstract = {In recent years, the prevalence of coronary artery disease (CAD) has become one of the leading causes of death around the world. Accurate stenosis detection of coronary arteries is crucial for timely treatment. Cardiologists use visual estimations when reading coronary angiography images to diagnose stenosis. As a result, they face various challenges which include high workloads, long processing times and human error. Computer-aided segmentation and classification of coronary arteries, as to whether stenosis is present or not, significantly reduces the workload of cardiologists and human errors caused by manual processes. Moreover, deep learning techniques have been shown to aid medical experts in diagnosing diseases using biomedical imaging. Thus, this study proposes the use of automatic segmentation of coronary arteries using U-Net, ResUNet-a, UNet++, models and classification using DenseNet201, EfficientNet-B0, Mobilenet-v2, ResNet101 and Xception models. In the case of segmentation, the comparative analysis of the three models has shown that U-Net achieved the highest score with a 0.8467 Dice score and 0.7454 Jaccard Index in comparison with UNet++ and ResUnet-a. Evaluation of the classification model’s performances has shown that DenseNet201 performed better than other pretrained models with 0.9000 accuracy, 0.9833 specificity, 0.9556 PPV, 0.7746 Cohen’s Kappa and 0.9694 Area Under the Curve (AUC).},
	language = {en},
	number = {13},
	urldate = {2024-10-11},
	journal = {Diagnostics},
	author = {Kaba, Şerife and Haci, Huseyin and Isin, Ali and Ilhan, Ahmet and Conkbayir, Cenk},
	month = jan,
	year = {2023},
	note = {Number: 13
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {U-Net, angiography, coronary arteries, coronary artery disease (CAD), mittel, pretrained models},
	pages = {2274},
}

@article{sharma_analysis_2018,
	series = {International {Conference} on {Computational} {Intelligence} and {Data} {Science}},
	title = {An {Analysis} {Of} {Convolutional} {Neural} {Networks} {For} {Image} {Classification}},
	volume = {132},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050918309335},
	doi = {10.1016/j.procs.2018.05.198},
	abstract = {This paper presents an empirical analysis of theperformance of popular convolutional neural networks (CNNs) for identifying objects in real time video feeds. The most popular convolution neural networks for object detection and object category classification from images are Alex Nets, GoogLeNet, and ResNet50. A variety of image data sets are available to test the performance of different types of CNN’s. The commonly found benchmark datasets for evaluating the performance of a convolutional neural network are anImageNet dataset, and CIFAR10, CIFAR100, and MNIST image data sets. This study focuses on analyzing the performance of three popular networks: Alex Net, GoogLeNet, and ResNet50. We have taken three most popular data sets ImageNet, CIFAR10, and CIFAR100 for our study, since, testing the performance of a network on a single data set does not reveal its true capability and limitations. It must be noted that videos are not used as a training dataset, they are used as testing datasets. Our analysis shows that GoogLeNet and ResNet50 are able to recognize objects with better precision compared to Alex Net. Moreover, theperformance of trained CNN’s vary substantially across different categories of objects and we, therefore, will discuss the possible reasons for this.},
	urldate = {2024-08-27},
	journal = {Procedia Computer Science},
	author = {Sharma, Neha and Jain, Vibhor and Mishra, Anju},
	month = jan,
	year = {2018},
	keywords = {CNN, Deep Learning, Neural network, Object classification, Object detection, sehr hoch!},
	pages = {377--384},
}

@article{voulodimos_deep_2018,
	title = {Deep {Learning} for {Computer} {Vision}: {A} {Brief} {Review}},
	volume = {2018},
	copyright = {Copyright © 2018 Athanasios Voulodimos et al.},
	issn = {1687-5273},
	shorttitle = {Deep {Learning} for {Computer} {Vision}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2018/7068349},
	doi = {10.1155/2018/7068349},
	abstract = {Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.},
	language = {en},
	number = {1},
	urldate = {2024-08-12},
	journal = {Computational Intelligence and Neuroscience},
	author = {Voulodimos, Athanasios and Doulamis, Nikolaos and Doulamis, Anastasios and Protopapadakis, Eftychios},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1155/2018/7068349},
	keywords = {hoch, sehr hoch!},
	pages = {7068349},
}

@article{zhao_deep_2019,
	title = {Deep learning and its applications to machine health monitoring},
	volume = {115},
	issn = {0888-3270},
	url = {https://www.sciencedirect.com/science/article/pii/S0888327018303108},
	doi = {10.1016/j.ymssp.2018.05.050},
	abstract = {Since 2006, deep learning (DL) has become a rapidly growing research direction, redefining state-of-the-art performances in a wide range of areas such as object recognition, image segmentation, speech recognition and machine translation. In modern manufacturing systems, data-driven machine health monitoring is gaining in popularity due to the widespread deployment of low-cost sensors and their connection to the Internet. Meanwhile, deep learning provides useful tools for processing and analyzing these big machinery data. The main purpose of this paper is to review and summarize the emerging research work of deep learning on machine health monitoring. After the brief introduction of deep learning techniques, the applications of deep learning in machine health monitoring systems are reviewed mainly from the following aspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and its variants including Deep Belief Network (DBN) and Deep Boltzmann Machines (DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). In addition, an experimental study on the performances of these approaches has been conducted, in which the data and code have been online. Finally, some new trends of DL-based machine health monitoring methods are discussed.},
	urldate = {2024-09-29},
	journal = {Mechanical Systems and Signal Processing},
	author = {Zhao, Rui and Yan, Ruqiang and Chen, Zhenghua and Mao, Kezhi and Wang, Peng and Gao, Robert X.},
	month = jan,
	year = {2019},
	keywords = {Big data, Deep learning, Machine health monitoring, hoch, sehr hoch!},
	pages = {213--237},
}

@misc{new_generation_DL,
	title = {A {Survey} on the {New} {Generation} of {Deep} {Learning} in {Image} {Processing} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/abstract/document/8917633},
	urldate = {2024-05-07},
	author = {Jiao, Licheng and Zhao, Jin},
	year = {2019},
	keywords = {sehr hoch!},
}

@article{zhao_object_2019,
	title = {Object {Detection} {With} {Deep} {Learning}: {A} {Review}},
	volume = {30},
	issn = {2162-2388},
	shorttitle = {Object {Detection} {With} {Deep} {Learning}},
	url = {https://ieeexplore.ieee.org/document/8627998},
	doi = {10.1109/TNNLS.2018.2876865},
	abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.},
	number = {11},
	urldate = {2024-08-12},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},
	month = nov,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Computer architecture, Deep learning, Feature extraction, Neural networks, Object detection, Task analysis, Training, hoch, neural network, object detection, sehr hoch!},
	pages = {3212--3232},
}

@inproceedings{qi_review_2020,
	address = {New York, NY, USA},
	series = {{MLMI} '20},
	title = {A {Review} on {Industrial} {Surface} {Defect} {Detection} {Based} on {Deep} {Learning} {Technology}},
	isbn = {978-1-4503-8834-4},
	url = {https://dl.acm.org/doi/10.1145/3426826.3426832},
	doi = {10.1145/3426826.3426832},
	abstract = {In recent years, with the rapid development of deep learning, computer vision technology based on convolutional neural network (CNN) is widely used in industrial fields. At present, surface defect detection by machine vision is one of the most mature applications of CNN in industry. This paper provides a comprehensive overview of deep learning in the field. First of all, we briefly introduce the major tasks of CNN in computer vision researches, including image classification, object detection, edge detection and image segmentation, which are frequently used techniques in surface defect inspection. After that, we describe in detail the applications of computer vision based on CNN models in a variety of industrial scenarios for surface defect detection tasks, which mainly cover the steel surface defect inspection, magnetic tile surface defect inspection, rail surface defect inspection, screen surface detect inspection, solar cell surface defect inspection, and some others. As an emerging representative of artificial intelligence technology, we believe that deep learning will gradually become one of the mainstream technologies for industrial vision in the future. Accordingly, this paper aims to present a reference and guidance for researchers in industry to apply the advanced technology of deep learning.},
	urldate = {2024-09-26},
	booktitle = {Proceedings of the 2020 3rd {International} {Conference} on {Machine} {Learning} and {Machine} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Qi, Shengxiang and Yang, Jiarong and Zhong, Zhenyi},
	year = {2020},
	keywords = {mittel},
	pages = {24--30},
}

@article{raschka_machine_2020,
	title = {Machine {Learning} in {Python}: {Main} {Developments} and {Technology} {Trends} in {Data} {Science}, {Machine} {Learning}, and {Artificial} {Intelligence}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	shorttitle = {Machine {Learning} in {Python}},
	url = {https://www.mdpi.com/2078-2489/11/4/193},
	doi = {10.3390/info11040193},
	abstract = {Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.},
	language = {en},
	number = {4},
	urldate = {2024-08-12},
	journal = {Information},
	author = {Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},
	month = apr,
	year = {2020},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {GPU computing, Python, data science, deep learning, machine learning, neural networks, sehr hoch!},
	pages = {193},
}

@article{tajbakhsh_embracing_2020,
	title = {Embracing imperfect datasets: {A} review of deep learning solutions for medical image segmentation},
	volume = {63},
	issn = {1361-8415},
	shorttitle = {Embracing imperfect datasets},
	url = {https://www.sciencedirect.com/science/article/pii/S136184152030058X},
	doi = {10.1016/j.media.2020.101693},
	abstract = {The medical imaging literature has witnessed remarkable progress in high-performing segmentation models based on convolutional neural networks. Despite the new performance highs, the recent advanced segmentation models still require large, representative, and high quality annotated datasets. However, rarely do we have a perfect training dataset, particularly in the field of medical imaging, where data and annotations are both expensive to acquire. Recently, a large body of research has studied the problem of medical image segmentation with imperfect datasets, tackling two major dataset limitations: scarce annotations where only limited annotated data is available for training, and weak annotations where the training data has only sparse annotations, noisy annotations, or image-level annotations. In this article, we provide a detailed review of the solutions above, summarizing both the technical novelties and empirical results. We further compare the benefits and requirements of the surveyed methodologies and provide our recommended solutions. We hope this survey article increases the community awareness of the techniques that are available to handle imperfect medical image segmentation datasets.},
	urldate = {2024-08-07},
	journal = {Medical Image Analysis},
	author = {Tajbakhsh, Nima and Jeyaseelan, Laura and Li, Qian and Chiang, Jeffrey N. and Wu, Zhihao and Ding, Xiaowei},
	month = jul,
	year = {2020},
	keywords = {And weak annotations, Imperfect dataset, Medical image segmentation, Noisy annotations, Scarce annotations, Sparse annotations, Unreliable annotations, sehr hoch!},
	pages = {101693},
}

@article{bhatt_image-based_2021,
	title = {Image-{Based} {Surface} {Defect} {Detection} {Using} {Deep} {Learning}: {A} {Review}},
	volume = {21},
	issn = {1530-9827},
	shorttitle = {Image-{Based} {Surface} {Defect} {Detection} {Using} {Deep} {Learning}},
	url = {https://doi.org/10.1115/1.4049535},
	doi = {10.1115/1.4049535},
	abstract = {Automatically detecting surface defects from images is an essential capability in manufacturing applications. Traditional image processing techniques are useful in solving a specific class of problems. However, these techniques do not handle noise, variations in lighting conditions, and backgrounds with complex textures. In recent times, deep learning has been widely explored for use in automation of defect detection. This survey article presents three different ways of classifying various efforts in literature for surface defect detection using deep learning techniques. These three ways are based on defect detection context, learning techniques, and defect localization and classification method respectively. This article also identifies future research directions based on the trends in the deep learning area.},
	number = {040801},
	urldate = {2024-08-12},
	journal = {Journal of Computing and Information Science in Engineering},
	author = {Bhatt, Prahar M. and Malhan, Rishi K. and Rajendran, Pradeep and Shah, Brual C. and Thakar, Shantanu and Yoon, Yeo Jung and Gupta, Satyandra K.},
	month = feb,
	year = {2021},
	keywords = {hoch},
}

@article{siddique_u-net_2021,
	title = {U-{Net} and {Its} {Variants} for {Medical} {Image} {Segmentation}: {A} {Review} of {Theory} and {Applications}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {U-{Net} and {Its} {Variants} for {Medical} {Image} {Segmentation}},
	url = {https://ieeexplore.ieee.org/abstract/document/9446143},
	doi = {10.1109/ACCESS.2021.3086020},
	abstract = {U-net is an image segmentation technique developed primarily for image segmentation tasks. These traits provide U-net with a high utility within the medical imaging community and have resulted in extensive adoption of U-net as the primary tool for segmentation tasks in medical imaging. The success of U-net is evident in its widespread use in nearly all major image modalities, from CT scans and MRI to X-rays and microscopy. Furthermore, while U-net is largely a segmentation tool, there have been instances of the use of U-net in other applications. Given that U-net's potential is still increasing, this narrative literature review examines the numerous developments and breakthroughs in the U-net architecture and provides observations on recent trends. We also discuss the many innovations that have advanced in deep learning and discuss how these tools facilitate U-net. In addition, we review the different image modalities and application areas that have been enhanced by U-net.},
	urldate = {2024-08-12},
	journal = {IEEE Access},
	author = {Siddique, Nahian and Paheding, Sidike and Elkin, Colin P. and Devabhaktuni, Vijay},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Biomedical imaging, Computer architecture, Convolution, Deep learning, Image segmentation, Logic gates, Three-dimensional displays, U-net, deep learning, hoch, neural network architecture, segmentation, sehr hoch!},
	pages = {82031--82057},
}

@article{zheng_recent_2021,
	title = {Recent advances in surface defect inspection of industrial products using deep learning techniques},
	volume = {113},
	issn = {1433-3015},
	url = {https://doi.org/10.1007/s00170-021-06592-8},
	doi = {10.1007/s00170-021-06592-8},
	abstract = {Manual surface inspection methods performed by quality inspectors do not satisfy the continuously increasing quality standards of industrial manufacturing processes. Machine vision provides a solution by using an automated visual inspection (AVI) system to perform quality inspection and remove defective products. Numerous studies and works have been conducted on surface inspection algorithms. With the advent of deep learning, a number of new algorithms have been developed for better inspection. In this paper, the state-of-the-art in surface defect inspection using deep learning is presented. In particular, we focus on the inspection of industrial products in semiconductor, steel, and fabric manufacturing processes. This work makes three contributions. First, we present the prior literature reviews on vision-based surface defect inspection and analyze the recent AVI-related hardware and software. Second, we review traditional surface defect inspection algorithms including statistical methods, spectral methods, model-based methods, and learning-based methods. Third, we investigate recent advances in deep learning-based inspection algorithms and present their applications in the steel, fabric, and semiconductor industries. Furthermore, we provide information on publicly available datasets containing surface image samples to facilitate the research on deep learning-based surface inspection.},
	language = {en},
	number = {1},
	urldate = {2024-05-23},
	journal = {The International Journal of Advanced Manufacturing Technology},
	author = {Zheng, Xiaoqing and Zheng, Song and Kong, Yaguang and Chen, Jie},
	month = mar,
	year = {2021},
	keywords = {Automated surface inspection, Automated visual inspection, Deep learning, Defect detection, Defect inspection, Machine vision, hoch},
	pages = {35--58},
}

@article{konovalenko_research_2022,
	title = {Research of {U}-{Net}-{Based} {CNN} {Architectures} for {Metal} {Surface} {Defect} {Detection}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-1702},
	url = {https://www.mdpi.com/2075-1702/10/5/327},
	doi = {10.3390/machines10050327},
	abstract = {The quality, wear and safety of metal structures can be controlled effectively, provided that surface defects, which occur on metal structures, are detected at the right time. Over the past 10 years, researchers have proposed a number of neural network architectures that have shown high efficiency in various areas, including image classification, segmentation and recognition. However, choosing the best architecture for this particular task is often problematic. In order to compare various techniques for detecting defects such as “scratch abrasion”, we created and investigated U-Net-like architectures with encoders such as ResNet, SEResNet, SEResNeXt, DenseNet, InceptionV3, Inception-ResNetV2, MobileNet and EfficientNet. The relationship between training validation metrics and final segmentation test metrics was investigated. The correlation between the loss function, the DSC, IoU, Recall, Precision and F1 validation metrics and DSC test metrics was calculated. Recognition accuracy was analyzed as affected by the optimizer during neural network training. In the context of this problem, neural networks trained using the stochastic gradient descent optimizer with Nesterov momentum were found to have the best generalizing properties. To select the best model during its training on the basis of the validation metrics, the main test metrics of recognition quality (Dice similarity coefficient) were analyzed depending on the validation metrics. The ResNet and DenseNet models were found to achieve the best generalizing properties for our task. The highest recognition accuracy was attained using the U-Net model with a ResNet152 backbone. The results obtained on the test dataset were DSC=0.9304 and IoU=0.9122.},
	language = {en},
	number = {5},
	urldate = {2024-08-12},
	journal = {Machines},
	author = {Konovalenko, Ihor and Maruschak, Pavlo and Brezinová, Janette and Prentkovskis, Olegas and Brezina, Jakub},
	month = may,
	year = {2022},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {CNN optimizer, image segmentation, metallurgy, strip surface, surface defect detection, visual inspection technology},
	pages = {327},
}

@article{minaee_image_2022,
	title = {Image {Segmentation} {Using} {Deep} {Learning}: {A} {Survey}},
	volume = {44},
	issn = {1939-3539},
	shorttitle = {Image {Segmentation} {Using} {Deep} {Learning}},
	url = {https://ieeexplore.ieee.org/document/9356353},
	doi = {10.1109/TPAMI.2021.3059968},
	abstract = {Image segmentation is a key task in computer vision and image processing with important applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among others, and numerous segmentation algorithms are found in the literature. Against this backdrop, the broad success of deep learning (DL) has prompted the development of new image segmentation approaches leveraging DL models. We provide a comprehensive review of this recent literature, covering the spectrum of pioneering efforts in semantic and instance segmentation, including convolutional pixel-labeling networks, encoder-decoder architectures, multiscale and pyramid-based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the relationships, strengths, and challenges of these DL-based segmentation models, examine the widely used datasets, compare performances, and discuss promising research directions.},
	number = {7},
	urldate = {2024-08-06},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Minaee, Shervin and Boykov, Yuri and Porikli, Fatih and Plaza, Antonio and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
	month = jul,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Computational modeling, Computer architecture, Deep learning, Generative adversarial networks, Image segmentation, Logic gates, Semantics, convolutional neural networks, deep learning, encoder-decoder models, generative models, hoch, instance segmentation, medical image segmentation, panoptic segmentation, recurrent models, sehr hoch!, semantic segmentation},
	pages = {3523--3542},
}

@article{mo_review_2022,
	title = {Review the state-of-the-art technologies of semantic segmentation based on deep learning},
	volume = {493},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231222000054},
	doi = {10.1016/j.neucom.2022.01.005},
	abstract = {The goal of semantic segmentation is to segment the input image according to semantic information and predict the semantic category of each pixel from a given label set. With the gradual intellectualization of modern life, more and more applications need to infer relevant semantic information from images for subsequent processing, such as augmented reality, autonomous driving, video surveillance, etc. This paper reviews the state-of-the-art technologies of semantic segmentation based on deep learning. Because semantic segmentation requires a large number of pixel-level annotations, in order to reduce the fine-grained requirements of annotation and reduce the economic and time cost of manual annotation, this paper studies the works on weakly-supervised semantic segmentation. In order to enhance the generalization ability and robustness of the segmentation model, this paper investigates the works on domain adaptation in semantic segmentation. Many types of sensors are usually equipped in some practical applications, such as autonomous driving and medical image analysis. In order to mine the association between multi-modal data and improve the accuracy of the segmentation model, this paper investigates the works based on multi-modal data fusion semantic segmentation. The real-time performance of the model needs to be considered in practical application. This paper analyzes the key factors affecting the real-time performance of the segmentation model and investigates the works on real-time semantic segmentation. Finally, this paper summarizes the challenges and promising research directions of semantic segmentation tasks based on deep learning.},
	urldate = {2024-08-12},
	journal = {Neurocomputing},
	author = {Mo, Yujian and Wu, Yan and Yang, Xinneng and Liu, Feilin and Liao, Yujun},
	month = jul,
	year = {2022},
	keywords = {Convolutional neural networks, Deep learning, Domain adaptation, Multi-modal fusion, Real-time, Semantic segmentation, Weakly-supervised, sehr hoch!},
	pages = {626--646},
}

@article{usamentiaga_automated_2022,
	title = {Automated {Surface} {Defect} {Detection} in {Metals}: {A} {Comparative} {Review} of {Object} {Detection} and {Semantic} {Segmentation} {Using} {Deep} {Learning}},
	volume = {58},
	issn = {1939-9367},
	shorttitle = {Automated {Surface} {Defect} {Detection} in {Metals}},
	url = {https://ieeexplore.ieee.org/abstract/document/9713940},
	doi = {10.1109/TIA.2022.3151560},
	abstract = {Automated surface defect detection is a challenging problem that has attracted major attention for decades. Traditional methods were designed using a pipeline of carefully designed operations. The resulting methods were complex systems, which were difficult to tune and adapt to different problems or data. A new approach to solving this problem has emerged recently: deep learning. This trend is motivated by two main factors: the increasing digitization of society, which makes it possible to record large datasets of labeled samples; and the availability of a large pool of computational resources. This work evaluates state-of-the-art deep-learning methods in object detection and semantic segmentation in the field of automated surface inspection in metals. Images acquired in the industry are affected by the conditions of the environment, including noise, dust, and vibrations, which is an additional challenge. Moreover, industrial inspection requires accuracy, but also robustness and speed. The selected methods are applied to different datasets of images that include the most common defects in metals and the performance is compared in terms of accuracy and speed. Results show exceptional accuracy at a fraction of the required processing time.},
	number = {3},
	urldate = {2024-08-12},
	journal = {IEEE Transactions on Industry Applications},
	author = {Usamentiaga, Rubén and Lema, Darío G. and Pedrayes, Oscar D. and Garcia, Daniel F.},
	month = may,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Industry Applications},
	keywords = {Deep learning, Image segmentation, Inspection, Object detection, Semantics, Steel, Surface treatment, defect detection, hoch, image processing, quality control, surface inspection},
	pages = {4203--4213},
}

@article{kaur_comprehensive_2023,
	title = {A comprehensive review of object detection with deep learning},
	volume = {132},
	issn = {1051-2004},
	url = {https://www.sciencedirect.com/science/article/pii/S1051200422004298},
	doi = {10.1016/j.dsp.2022.103812},
	abstract = {In the realm of computer vision, Deep Convolutional Neural Networks (DCNNs) have demonstrated excellent performance. Video Processing, Object Detection, Image Segmentation, Image Classification, Speech Recognition and Natural Language Processing are some of the application areas of CNN. Object detection is the most crucial and challenging task of computer vision. It has numerous applications in the field of security, military, transportation and medical sciences. In this review, object detection and its different aspects have been covered in detail. With the gradual increase in the evolution of deep learning algorithms for detecting objects, a significant improvement in the performance of object detection models has been observed. However, this does not imply that the conventional object detection methods, which had been evolving for decades prior to the emergence of deep learning, had become outdated. There are some cases where conventional methods with global features are superior choice. This review paper starts with a quick overview of object detection followed by object detection frameworks, backbone convolutional neural network, and an overview of common datasets along with the evaluation metrics. Object detection problems and applications are also studied in detail. Some future research challenges in designing deep neural networks are discussed. Lastly, the performance of object detection models on PASCAL VOC and MS COCO datasets is compared and conclusions are drawn.},
	urldate = {2024-10-04},
	journal = {Digital Signal Processing},
	author = {Kaur, Ravpreet and Singh, Sarbjeet},
	month = jan,
	year = {2023},
	keywords = {Computer vision, Conventional methods, Deep convolutional neural network, Deep learning, Object detection, sehr hoch!},
	pages = {103812},
}

@article{liu_deep_2023-1,
	title = {Deep learning in image segmentation for mineral production: {A} review},
	volume = {180},
	issn = {0098-3004},
	shorttitle = {Deep learning in image segmentation for mineral production},
	url = {https://www.sciencedirect.com/science/article/pii/S0098300423001590},
	doi = {10.1016/j.cageo.2023.105455},
	abstract = {Mineral image segmentation is widely used in mining, sorting, exploration, composition analysis, and other production works. The burgeoning field of deep learning provides preferred solutions for mineral image segmentation. We present a review of recent literature in this direction, covering the module components, encoder-decoders architecture, representative networks, mineral image datasets, performance metrics, and state-of-the-art models. In the application performance survey, the review contents include mineral type, image type, image resolution, image data quantity, architecture selection, and encoder network construction, as well as summarizes the advantages of deep learning-based mineral image segmentation methods. We conducted small-scale experiments for the current mainstream architectures and visualize the segmentation results for performance comparison. We also investigated the application challenges and bottlenecks of deep learning-based methods, propose several innovative directions, and discuss promising future applications.},
	urldate = {2024-08-19},
	journal = {Computers \& Geosciences},
	author = {Liu, Yang and Wang, Xueyi and Zhang, Zelin and Deng, Fang},
	month = nov,
	year = {2023},
	keywords = {Application performance survey, Deep learning, Encoder-decoders architecture, Intelligent mineral industry, Mineral image segmentation},
	pages = {105455},
}

@inproceedings{polte_intelligente_2023,
	address = {Wiesbaden},
	title = {Intelligente {Qualitätssicherung} im industriellen {Produktionsprozess} unter {Verwendung} von {KI}-{Algorithmen}},
	isbn = {978-3-658-40588-5},
	doi = {10.1007/978-3-658-40588-5_7},
	abstract = {In diesem Beitrag werden intelligente Qualitätssicherungslösungen für die automatisierte Erkennung verschiedener Fehlerklassen im industriellen Fertigungsprozess auf Basis optischer Bilderfassung, intelligenter digitaler Bildverarbeitung sowie Verfahren der Künstlichen Intelligenz vorgestellt. Hierbei werden schnelle automatisierte QS-Lösungen, sowohl für den Kunststoffspritzguss von Bauteilen im Automobilbau auf der Basis Robotik-assistierter Farbbildaufnahmen, als auch für die Metalloberflächenanalyse im Fräsbearbeitungsprozess auf der Basis von Farbbildern, aufgezeigt.},
	language = {de},
	booktitle = {Nachhaltiges {Qualitätsdatenmanagement}},
	publisher = {Springer Fachmedien},
	author = {Polte, Galina and Anding, Katharina and Liu, Kun and Garten, Daniel and Wunsch, Lennard and Notni, Gunther},
	editor = {Gröger, Sophie},
	year = {2023},
	keywords = {Convolutional Neural Network, Data Augmentation, Deep Learning, Generative Adversarial Networks, Intelligente Qualitätssicherung},
	pages = {120--138},
}

@article{saberironaghi_defect_2023,
	title = {Defect {Detection} {Methods} for {Industrial} {Products} {Using} {Deep} {Learning} {Techniques}: {A} {Review}},
	volume = {16},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1999-4893},
	shorttitle = {Defect {Detection} {Methods} for {Industrial} {Products} {Using} {Deep} {Learning} {Techniques}},
	url = {https://www.mdpi.com/1999-4893/16/2/95},
	doi = {10.3390/a16020095},
	abstract = {Over the last few decades, detecting surface defects has attracted significant attention as a challenging task. There are specific classes of problems that can be solved using traditional image processing techniques. However, these techniques struggle with complex textures in backgrounds, noise, and differences in lighting conditions. As a solution to this problem, deep learning has recently emerged, motivated by two main factors: accessibility to computing power and the rapid digitization of society, which enables the creation of large databases of labeled samples. This review paper aims to briefly summarize and analyze the current state of research on detecting defects using machine learning methods. First, deep learning-based detection of surface defects on industrial products is discussed from three perspectives: supervised, semi-supervised, and unsupervised. Secondly, the current research status of deep learning defect detection methods for X-ray images is discussed. Finally, we summarize the most common challenges and their potential solutions in surface defect detection, such as unbalanced sample identification, limited sample size, and real-time processing.},
	language = {en},
	number = {2},
	urldate = {2024-09-19},
	journal = {Algorithms},
	author = {Saberironaghi, Alireza and Ren, Jing and El-Gindy, Moustafa},
	month = feb,
	year = {2023},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute
Key: defect\_detection\_2023\_review},
	keywords = {deep learning, defect detection, defect detection for X-ray images, defect recognition, sehr hoch!, surface defect detection},
	pages = {95},
}

@article{singh_automated_2023,
	title = {Automated surface defect detection framework using machine vision and convolutional neural networks},
	volume = {34},
	issn = {1572-8145},
	url = {https://doi.org/10.1007/s10845-021-01878-w},
	doi = {10.1007/s10845-021-01878-w},
	abstract = {Machine vision-based inspection technologies are gaining considerable importance for automated monitoring and quality control of manufactured products in recent years due to the advent of Industry 4.0. The involvement of advanced deep learning methods is a significant factor contributing to the advent of robust vision-based solutions for improving inspection accuracy at a significantly lower cost in manufacturing industries. The requirement of computational resources and large training datasets hinders the deployment of these solutions to manufacturing shop floors. The present research work develops an image-based framework considering pre-trained Convolutional Neural Network (CNN), ResNet-101 to detect surface defects with the minimum training datasets and computational requirements. The outcomes of the proposed framework are substantiated through a case study of detecting commonly observed surface defects during the centerless grinding of tapered rollers. The image datasets consisting of standard tapered rollers and three common defect classes are captured and enriched further with the help of the data augmentation technique. The present work employs ResNet-101 for feature extraction combined with and multi-class Support Vector Machine (SVM) as a classifier to detect defective images. The effects of the feature extraction layer (fc1000) and pooling layer (pool5) activation are explored to achieve the desired prediction abilities. The testing trials demonstrate that the proposed framework effectively performs image classification, achieving 100\% precision for the ‘Good’ class components. The study showed that the proposed approach could overcome the requirements of large training datasets and higher computational power for deep learning models. The proposed system can be of significant importance for Micro, Small, and Medium Enterprises (MSMEs) and Small and Medium-sized Enterprises (SMEs) as an alternative to conventional labor-intensive manual inspection techniques.},
	language = {en},
	number = {4},
	urldate = {2024-09-19},
	journal = {Journal of Intelligent Manufacturing},
	author = {Singh, Swarit Anand and Desai, K. A.},
	month = apr,
	year = {2023},
	keywords = {Artificial Intelligence, Centerless grinding, Deep learning, Machine vision, ResNet-101, Surface defect detection, hoch},
	pages = {1995--2011},
}

@article{yu_challenges_2023,
	title = {Challenges and opportunities of deep learning-based process fault detection and diagnosis: a review},
	volume = {35},
	issn = {1433-3058},
	shorttitle = {Challenges and opportunities of deep learning-based process fault detection and diagnosis},
	url = {https://doi.org/10.1007/s00521-022-08017-3},
	doi = {10.1007/s00521-022-08017-3},
	abstract = {Process fault detection and diagnosis (FDD) is a predominant task to ensure product quality and process reliability in modern industrial systems. Those traditional FDD techniques are largely based on diagnostic experience. These methods have met significant challenges with immense expansion of plant scale and large numbers of process variables. Recently, deep learning has become the newest trends in process control. The upsurge of deep neural networks (DNNs) in leaning highly discriminative features from complicated process data has provided practitioners with effective process monitoring tools. This paper is to present a review and full developing route of deep learning-based FDD in complex process industries. Firstly, the nature of traditional data projection-based and machine learning-based FDD methods is discussed in process FDD. Secondly, the characteristics of deep learning and their applications in process FDD are illustrated. Thirdly, these typical deep learning techniques, e.g., transfer learning, generative adversarial network, capsule network, graph neural network, are presented for process FDD. These DNNs will effectively solve these problems of fault detection, fault classification, and fault isolation in process. Finally, the developing route of DNN-based process FDD techniques is highlighted for future work.},
	language = {en},
	number = {1},
	urldate = {2024-07-29},
	journal = {Neural Computing and Applications},
	author = {Yu, Jianbo and Zhang, Yue},
	month = jan,
	year = {2023},
	keywords = {Deep learning, Feature learning, Process fault detection, Process fault diagnosis, mittel},
	pages = {211--252},
}

@article{azad_medical_2024,
	title = {Medical {Image} {Segmentation} {Review}: {The} {Success} of {U}-{Net}},
	issn = {1939-3539},
	shorttitle = {Medical {Image} {Segmentation} {Review}},
	url = {https://ieeexplore.ieee.org/document/10643318/?arnumber=10643318},
	doi = {10.1109/TPAMI.2024.3435571},
	abstract = {Automatic medical image segmentation is a crucial topic in the medical domain and successively a critical counterpart in the computer-aided diagnosis paradigm. U-Net is the most widespread image segmentation architecture due to its flexibility, optimized modular design, and success in all medical image modalities. Over the years, the U-Net model has received tremendous attention from academic and industrial researchers who have extended it to address the scale and complexity created by medical tasks. These extensions are commonly related to enhancing the U-Net's backbone, bottleneck, or skip connections, or including representation learning, or combining it with a Transformer architecture, or even addressing probabilistic prediction of the segmentation map. Having a compendium of different previously proposed U-Net variants makes it easier for machine learning researchers to identify relevant research questions and understand the challenges of the biological tasks that challenge the model. In this work, we discuss the practical aspects of the U-Net model and organize each variant model into a taxonomy. Moreover, to measure the performance of these strategies in a clinical application, we propose fair evaluations of some unique and famous designs on well-known datasets. Furthermore, we provide a comprehensive implementation library with trained models. In addition, for ease of future studies, we created an online list of U-Net papers with their possible official implementation. All information is gathered in a GitHub repository https://github.com/NITR098/Awesome-U-Net.},
	urldate = {2024-09-24},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Azad, Reza and Aghdam, Ehsan Khodapanah and Rauland, Amelie and Jia, Yiwei and Avval, Atlas Haddadi and Bozorgpour, Afshin and Karimijafarbigloo, Sanaz and Cohen, Joseph Paul and Adeli, Ehsan and Merhof, Dorit},
	year = {2024},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Biomedical imaging, Computer architecture, Convolutional Neural Network, Deep Learning, Feature extraction, Image segmentation, Medical Image Segmentation, Task analysis, Taxonomy, Transformer, Transformers, U-Net, sehr hoch!},
	pages = {1--20},
}

@article{manakitsa_review_2024,
	title = {A {Review} of {Machine} {Learning} and {Deep} {Learning} for {Object} {Detection}, {Semantic} {Segmentation}, and {Human} {Action} {Recognition} in {Machine} and {Robotic} {Vision}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7080},
	url = {https://www.mdpi.com/2227-7080/12/2/15},
	doi = {10.3390/technologies12020015},
	abstract = {Machine vision, an interdisciplinary field that aims to replicate human visual perception in computers, has experienced rapid progress and significant contributions. This paper traces the origins of machine vision, from early image processing algorithms to its convergence with computer science, mathematics, and robotics, resulting in a distinct branch of artificial intelligence. The integration of machine learning techniques, particularly deep learning, has driven its growth and adoption in everyday devices. This study focuses on the objectives of computer vision systems: replicating human visual capabilities including recognition, comprehension, and interpretation. Notably, image classification, object detection, and image segmentation are crucial tasks requiring robust mathematical foundations. Despite the advancements, challenges persist, such as clarifying terminology related to artificial intelligence, machine learning, and deep learning. Precise definitions and interpretations are vital for establishing a solid research foundation. The evolution of machine vision reflects an ambitious journey to emulate human visual perception. Interdisciplinary collaboration and the integration of deep learning techniques have propelled remarkable advancements in emulating human behavior and perception. Through this research, the field of machine vision continues to shape the future of computer systems and artificial intelligence applications.},
	language = {en},
	number = {2},
	urldate = {2024-08-12},
	journal = {Technologies},
	author = {Manakitsa, Nikoleta and Maraslidis, George S. and Moysis, Lazaros and Fragulis, George F.},
	month = feb,
	year = {2024},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial intelligence, computer vision, deep learning, image processing, machine learning, machine vision, mechatronics, mittel, object classification, object detection, object segmentation, pattern recognition, robotics},
	pages = {15},
}

@article{xiao_transformers_2023,
	title = {Transformers in medical image segmentation: {A} review},
	volume = {84},
	issn = {1746-8094},
	shorttitle = {Transformers in medical image segmentation},
	url = {https://www.sciencedirect.com/science/article/pii/S1746809423002240},
	doi = {10.1016/j.bspc.2023.104791},
	abstract = {Background and Objectives:
Transformer is a model relying entirely on self-attention which has a wide range of applications in the field of natural language processing. Researchers are beginning to focus on the transformer in medical images due to the past few years having seen the rapid development of transformer in many vision fields such as vision transformer (ViT) and Swin transformer. In the last year, moreover, many scholars have applied transformer to medical image segmentation and have achieved good segmentation results. Transformer-based medical image segmentation has become one of the hot spots in this field. The purpose of this work is to categorize and review the segmentation methods of Unet-based transformer and other model based transformer in medical images.
Methods:
This paper summarizes the transformer-based segmentation models in the abdominal organs, heart, brain, and lung based on the relevant studies in the last two years. We described and analyzed the model structure including the position of the transformer in the model, the changes made by scholars to transformer and the combination with the model. In this work, the segmentation performance results based on Dice evaluation metrics are compared.
Results:
Through the help of 93 references, we find that researchers prefer to use Unet-based transformer models than others and place the transformer structure in the encoder. These new models improve the segmentation performance compared with U-Net and other segmentation models. However, there are not many related studies on lungs, which points to a new way for future research.
Conclusions:
We found that the combination of U-Net and transformer is more suitable for segmentation. In future research on medical image segmentation, researchers can use a suitable transformer-based segmentation method or modify the transformer structure according to the segmentation requirements. We hope that this work will be helpful for improvements of the transformer to solve clinical problems in medicine.},
	urldate = {2024-08-19},
	journal = {Biomedical Signal Processing and Control},
	author = {Xiao, Hanguang and Li, Li and Liu, Qiyuan and Zhu, Xiuhong and Zhang, Qihang},
	month = jul,
	year = {2023},
	keywords = {3D segmentation, Medical image, Segmentation analysis, Transformer, hoch},
	pages = {104791},
}

@article{zou_object_2023,
	title = {Object {Detection} in 20 {Years}: {A} {Survey}},
	volume = {111},
	issn = {1558-2256},
	shorttitle = {Object {Detection} in 20 {Years}},
	url = {https://ieeexplore.ieee.org/abstract/document/10028728},
	doi = {10.1109/JPROC.2023.3238524},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Over the past two decades, we have seen a rapid technological evolution of object detection and its profound impact on the entire computer vision field. If we consider today’s object detection technique as a revolution driven by deep learning, then, back in the 1990s, we would see the ingenious thinking and long-term perspective design of early computer vision. This article extensively reviews this fast-moving research field in the light of technical evolution, spanning over a quarter-century’s time (from the 1990s to 2022). A number of topics have been covered in this article, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speedup techniques, and recent state-of-the-art detection methods.},
	number = {3},
	urldate = {2024-10-04},
	journal = {Proceedings of the IEEE},
	author = {Zou, Zhengxia and Chen, Keyan and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	month = mar,
	year = {2023},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Computer vision, Convolutional neural networks, Deep learning, Detectors, Feature extraction, Object detection, convolutional neural networks (CNNs), deep learning, object detection, sehr hoch!, technical evolution},
	pages = {257--276},
}

@incollection{aziz_verwendung_2024,
	address = {Wiesbaden},
	title = {Verwendung von {Deep} {Learning} {Methoden} zur {Erkennung} und {Verfolgung} von {Objekten} bei {Inspektions}- und {Montageaufgaben}},
	isbn = {978-3-658-42796-2},
	url = {https://doi.org/10.1007/978-3-658-42796-2_11},
	abstract = {Im Bausektor bietet Building Information Modeling (BIM) in Kombination mit künstlicher Intelligenz als Querschnittstechnologie verschiedenste Anwendungsszenarien im gesamten Lebenszyklus eines Bauwerks. Besonders in der Ausführungs- und Betriebsphase eines Gebäudes lassen sich mit KI-gestützten Verfahren viele Prozesse verbessern und automatisieren, die aufgrund des manuellen Aufwands von Fachpersonal oft fehleranfällig und kostspielig sind. Dazu gehören unter anderem wiederkehrende Inspektions- und Montageaufgaben. Unter Verwendung von Bildern als auswertbare Eingabequellen und Computer-Vision-Methoden werden in diesem Beitrag mehrere Studien vorgestellt, die sich speziell auf Brandschutz- und Konstruktionsaufgaben mittels Deep-Learning-basierter Bild- und Echtzeiterkennungsmethoden konzentrieren.},
	language = {de},
	urldate = {2024-10-09},
	booktitle = {Künstliche {Intelligenz} im {Bauwesen} : {Grundlagen} und {Anwendungsfälle}},
	publisher = {Springer Fachmedien},
	author = {Aziz, Angelina and Gard, Niklas and Eisert, Peter and König, Markus and Hilsmann, Anna},
	editor = {Haghsheno, Shervin and Satzger, Gerhard and Lauble, Svenja and Vössing, Michael},
	year = {2024},
	doi = {10.1007/978-3-658-42796-2_11},
	pages = {189--203},
}

@incollection{schonfelder_automatische_2024,
	address = {Wiesbaden},
	title = {Automatische {Extraktion} von geometrischer und semantischer {Information} aus gescannten {Grundriss}-{Zeichnungen}},
	isbn = {978-3-658-42796-2},
	url = {https://doi.org/10.1007/978-3-658-42796-2_8},
	abstract = {Dieses Kapitel gibt einen Überblick über verschiedene Aspekte der Grundrissverarbeitung mit maschinellem Lernen, skizziert den aktuellen Stand der Forschung und veranschaulicht einige im Projekt BIMKIT entwickelte Methoden anhand von Beispielgrundrissen. Der Hauptbeitrag besteht darin, die Potenziale bereits bestehender Verfahren aufzuzeigen, insbesondere der Verfahren, die im Rahmen von BIMKIT entwickelt wurden, und darin, Aspekte des Forschungsbereichs aufzuzeigen, die bisher kaum behandelt werden.},
	language = {de},
	urldate = {2024-10-09},
	booktitle = {Künstliche {Intelligenz} im {Bauwesen} : {Grundlagen} und {Anwendungsfälle}},
	publisher = {Springer Fachmedien},
	author = {Schönfelder, Phillip and Fröml, Heinrich and Freiny, Julius and Barreiro, Aleixo Cambeiro and Hilsmann, Anna and Eisert, Peter and König, Markus},
	editor = {Haghsheno, Shervin and Satzger, Gerhard and Lauble, Svenja and Vössing, Michael},
	year = {2024},
	doi = {10.1007/978-3-658-42796-2_8},
	pages = {137--151},
}

@incollection{collins_bestandserfassung_2024,
	address = {Wiesbaden},
	title = {Bestandserfassung mithilfe von {Computer} {Vision} {Methoden}},
	isbn = {978-3-658-42796-2},
	url = {https://doi.org/10.1007/978-3-658-42796-2_18},
	abstract = {In diesem Kapitel werden verschiedene Ansätze für die Bestandserfassung mithilfe von ComputerVision (CV) Methoden dargestellt. Verschiedene Subdomänen von CV, wie beispielsweise semantische oder Instanz-Segmentierung, unterstützen bei der automatischen Anreicherung von semantischen Informationen basierend auf verschiedenen Eingangsdaten. Im ersten Anwendungsfall werden 2D-Zeichnung als Datenquelle verwendet, um geometrische As-Designed-Modelle zu rekonstruieren. Als zweite wichtige Datenquelle werden im zweiten Anwendungsfall räumlich-visuelle Bestandsaufnahmen (Punktwolken und Bilder) betrachtet und deren semantische Extraktionsmethoden vorgestellt.},
	language = {de},
	urldate = {2024-10-09},
	booktitle = {Künstliche {Intelligenz} im {Bauwesen} : {Grundlagen} und {Anwendungsfälle}},
	publisher = {Springer Fachmedien},
	author = {Collins, Fiona and Noichl, Florian and Pan, Yuandong and Carrara, Andrea and Mafipour, M. Saeed and Forth, Kasimir and Borrmann, André},
	editor = {Haghsheno, Shervin and Satzger, Gerhard and Lauble, Svenja and Vössing, Michael},
	year = {2024},
	doi = {10.1007/978-3-658-42796-2_18},
	pages = {309--326},
}

@article{zhang_understanding_2021,
	title = {Understanding deep learning (still) requires rethinking generalization},
	volume = {64},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3446776},
	doi = {10.1145/3446776},
	abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training.
            Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice.
            We interpret our experimental findings by comparison with traditional models.
            We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.},
	language = {en},
	number = {3},
	urldate = {2024-10-04},
	journal = {Communications of the ACM},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	month = mar,
	year = {2021},
	pages = {107--115},
}

@misc{vipernet_about_nodate,
	title = {About {Viper}.{NET} — {Viper}.{NET} 6.5.5.0 documentation},
	url = {https://viper.vision/01_introduction/about},
	urldate = {2024-09-26},
	author = {Viper.Net},
}

@misc{cognex_grundlagen_nodate,
	title = {Grundlagen der industriellen {Bildverarbeitung}: {Definitionen}, {Anwendungen} und {Vorteile} {\textbar} {Cognex}},
	url = {https://www.cognex.com/de-de/what-is/machine-vision},
	urldate = {2024-09-25},
	author = {Cognex},
}

@article{cheng_retinanet_2021,
	title = {{RetinaNet} {With} {Difference} {Channel} {Attention} and {Adaptively} {Spatial} {Feature} {Fusion} for {Steel} {Surface} {Defect} {Detection}},
	volume = {70},
	issn = {1557-9662},
	url = {https://ieeexplore.ieee.org/abstract/document/9270024?casa_token=rYUODf-kDVYAAAAA:4xGuHHgvtS4Kn5tyrnrwqbpkw2_gX1cTkSjJtBauF75mbxtd-qDe0YsvwhZhIbQxdF87h1kMNg},
	doi = {10.1109/TIM.2020.3040485},
	abstract = {Surface defect detection of products is an important process to guarantee the quality of industrial production. A defect detection task aims to identify the specific category and precise position of defect in an image. It is hard to take into account the accuracy of both, which makes it be challenging in practice. In this study, a new deep neural network (DNN), RetinaNet with difference channel attention and adaptively spatial feature fusion (DEA\_RetinaNet), is proposed for steel surface defect detection. First, a differential evolution search-based anchor optimization is performed to improve the detection accuracy of DEA\_RetinaNet. Second, a novel channel attention mechanism is embedded in DEA\_RetinaNet to reduce information loss. Finally, the adaptive spatial feature fusion (ASFF) module is used for an effective fusion of shallow and deep features extracted by convolutional kernels. The experimental results on a steel surface defect data set (NEU-DET) show that DEA\_RetinaNet achieved 78.25 mAP and improved by 2.92\% over RetinaNet. It has better recognition performance compared with other famous DNN-based detectors.},
	urldate = {2024-09-29},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Cheng, Xun and Yu, Jianbo},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Instrumentation and Measurement},
	keywords = {Anchor optimization, Detectors, Feature extraction, Proposals, Steel, Support vector machines, Surface treatment, Task analysis, deep neural network (DNN), defect detection, feature fusion, product surface defect},
	pages = {1--11},
}

@article{rong-qiang_fabric_2021,
	title = {Fabric {Defect} {Detection} {Method} {Based} on {Improved} {U}-{Net}},
	volume = {1948},
	issn = {1742-6596},
	url = {https://dx.doi.org/10.1088/1742-6596/1948/1/012160},
	doi = {10.1088/1742-6596/1948/1/012160},
	abstract = {Computer vision builds a connection between image processing and industrials, bringing modern perception to the automated industrials. At the same time, defect detection based on deep learning has played an important role in automated detection. In this paper, an improved convolutional neural network CU-Net for fabric defect detection is proposed. In this method, the classical U-Net network was improved. On the basis of network size compression, attention mechanism is introduced and a new compound loss function is used for training. Using the public AITEX defect fabric data set as the test sample, the experimental result shows that the accuracy and recall of the proposed method are 98.3\% and 92.7\%, respectively. Compared with the highest scores of other detection methods, they are improved by 4.8\% and 2.3\%, which improves the detection accuracy of fabric defect significantly.},
	language = {en},
	number = {1},
	urldate = {2024-09-29},
	journal = {Journal of Physics: Conference Series},
	author = {Rong-qiang, Liu and Ming-hui, Li and Jia-chen, Shi and Yi-bin, Liang},
	month = jun,
	year = {2021},
	note = {Publisher: IOP Publishing},
	pages = {012160},
}

@misc{noauthor_deep_nodate,
	title = {Deep {Learning}},
	url = {https://www.deeplearningbook.org/},
	urldate = {2024-09-26},
}

@article{sultana_evolution_2020,
	title = {Evolution of {Image} {Segmentation} using {Deep} {Convolutional} {Neural} {Network}: {A} {Survey}},
	volume = {201-202},
	issn = {0950-7051},
	shorttitle = {Evolution of {Image} {Segmentation} using {Deep} {Convolutional} {Neural} {Network}},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120303464},
	doi = {10.1016/j.knosys.2020.106062},
	abstract = {From the autonomous car driving to medical diagnosis, the requirement of the task of image segmentation is everywhere. Segmentation of an image is one of the indispensable tasks in computer vision. This task is comparatively complicated than other vision tasks as it needs low-level spatial information. Basically, image segmentation can be of two types: semantic segmentation and instance segmentation. The combined version of these two basic tasks is known as panoptic segmentation. In the recent era, the success of deep convolutional neural networks (CNN) has influenced the field of segmentation greatly and gave us various successful models to date. In this survey, we are going to take a glance at the evolution of both semantic and instance segmentation work based on CNN. We have also specified comparative architectural details of some state-of-the-art models and discuss their training details to present a lucid understanding of hyper-parameter tuning of those models. We have also drawn a comparison among the performance of those models on different datasets. Lastly, we have given a glimpse of some state-of-the-art panoptic segmentation models.},
	urldate = {2024-09-25},
	journal = {Knowledge-Based Systems},
	author = {Sultana, Farhana and Sufian, Abu and Dutta, Paramartha},
	month = aug,
	year = {2020},
	keywords = {Convolutional neural network, Deep learning, Instance segmentation, Panoptic segmentation, Semantic segmentation, Survey},
	pages = {106062},
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation {Applied} to {Handwritten} {Zip} {Code} {Recognition}},
	volume = {1},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/1/4/541-551/5515},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	language = {en},
	number = {4},
	urldate = {2024-09-23},
	journal = {Neural Computation},
	author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	month = dec,
	year = {1989},
	pages = {541--551},
}

@phdthesis{lei_data-driven_2023,
	type = {thesis},
	title = {Data-driven {Image} {Segmentation} of {Complex} {Microstructures} with {Deep} {Learning}},
	url = {https://kilthub.cmu.edu/articles/thesis/Data-driven_Image_Segmentation_of_Complex_Microstructures_with_Deep_Learning/22266406/1},
	abstract = {In quantitative microscopy, image segmentation plays a central role in quantitative measurements and analyses of microstructure constituents. Developing effective and efficient methods for automating the segmentation process is highly valued for materials research and manufacturing. Conventional image processing-based segmentation methods reach their limits in handling complicated microstructures and often require sophisticated processing pipelines. The cutting-edge data-driven deep learning methods have made huge breakthroughs in image-based tasks and they provide new possibilities for advanced microstructure image segmentation. In this thesis, we demonstrate that deep learning methods can be successfully applied to microstructure image segmentation tasks with great advantages in automation, performance and generality. We demonstrate the capabilities of deep learning methods in two different problems: (1) segmentation of complex multi-constituents microstructures in ultrahigh carbon steel and (2) segmentation of low-contrast lathshaped bainite in complex phase steel. General guidelines and strategies for tackling such tasks are discussed. To alleviate annotation cost and achieve high efficiency for practical applications, we develop our deep learning models in a semi-supervised manner with a significantly reduced amount of annotated data. An automated training image selection algorithm is proposed and we demonstrate in two steel microstructure segmentation datasets that deep learning models trained by one or a few images are competitive with fully-supervised models using 4 times more training images.},
	language = {en},
	urldate = {2024-09-19},
	school = {Carnegie Mellon University},
	author = {Lei, Bo},
	month = mar,
	year = {2023},
	doi = {10.1184/R1/22266406.v1},
}

@article{leclerc_arrastia_deeply_2021,
	title = {Deeply {Supervised} {UNet} for {Semantic} {Segmentation} to {Assist} {Dermatopathological} {Assessment} of {Basal} {Cell} {Carcinoma}},
	volume = {7},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/7/4/71},
	doi = {10.3390/jimaging7040071},
	abstract = {Accurate and fast assessment of resection margins is an essential part of a dermatopathologist’s clinical routine. In this work, we successfully develop a deep learning method to assist the dermatopathologists by marking critical regions that have a high probability of exhibiting pathological features in whole slide images (WSI). We focus on detecting basal cell carcinoma (BCC) through semantic segmentation using several models based on the UNet architecture. The study includes 650 WSI with 3443 tissue sections in total. Two clinical dermatopathologists annotated the data, marking tumor tissues’ exact location on 100 WSI. The rest of the data, with ground-truth sectionwise labels, are used to further validate and test the models. We analyze two different encoders for the first part of the UNet network and two additional training strategies: (a) deep supervision, (b) linear combination of decoder outputs, and obtain some interpretations about what the network’s decoder does in each case. The best model achieves over 96\%, accuracy, sensitivity, and specificity on the Test set.},
	language = {en},
	number = {4},
	urldate = {2024-08-28},
	journal = {Journal of Imaging},
	author = {Le’Clerc Arrastia, Jean and Heilenkötter, Nick and Otero Baguer, Daniel and Hauberg-Lotte, Lena and Boskamp, Tobias and Hetzer, Sonja and Duschner, Nicole and Schaller, Jörg and Maass, Peter},
	month = apr,
	year = {2021},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {UNet, basal cell carcinoma, deep learning, dermatopathology, digital pathology, skin cancer, whole slide image},
	pages = {71},
}

@article{jiang_masked_2023,
	title = {Masked {Swin} {Transformer} {Unet} for {Industrial} {Anomaly} {Detection}},
	volume = {19},
	issn = {1941-0050},
	url = {https://ieeexplore.ieee.org/abstract/document/9858596?casa_token=HZW2g0jcEMQAAAAA:gyyXC516ji2QrvT81Z4Z1HUTQ3sgFLDwQ5yiYyGwIvq9Ebq_7fniuKhwcK-ajD7YwPiK7kVz},
	doi = {10.1109/TII.2022.3199228},
	abstract = {The intelligent detection process for industrial anomalies employs artificial intelligence methods to classify images that deviate from a normal appearance. Traditional convolutional neural network (CNN)-based anomaly detection algorithms mainly use the network to restructure abnormal areas and detect anomalies by calculating the errors between the original image and reconstructed image. However, the traditional CNNs struggle to extract global context information, resulting in poor anomaly detection performance. Thus, a masked Swin Transformer Unet (MSTUnet) for anomaly detection is proposed. To solve the problem of insufficient abnormal samples in the training phase, an anomaly simulation and mask strategy is first applied on anomaly-free samples to generate a simulated anomaly and, then, the Swin Transformer's powerful global learning ability is used to inpaint the masked area. Finally, a convolution-based Unet network is used for end-to-end anomaly detection. Experimental results on industrial dataset MVTec AD show that MSTUnet achieves superior anomaly detection and localization performance.},
	number = {2},
	urldate = {2024-08-28},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Jiang, Jielin and Zhu, Jiale and Bilal, Muhammad and Cui, Yan and Kumar, Neeraj and Dou, Ruihan and Su, Feng and Xu, Xiaolong},
	month = feb,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Industrial Informatics},
	keywords = {Anomaly detection, Data models, Feature extraction, Image reconstruction, Information science, Swin Transformer, Training, Transformers, Unet, inpainting},
	pages = {2200--2209},
}

@article{rebuf_data_nodate,
	title = {Data {Augmentation} {Can} {Improve} {Robustness}},
	abstract = {Adversarial training suffers from robust overﬁtting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on reducing robust overﬁtting by using common data augmentation schemes. We demonstrate that, contrary to previous ﬁndings, when combined with model weight averaging, data augmentation can signiﬁcantly boost robust accuracy. Furthermore, we compare various data augmentations techniques and observe that spatial composition techniques work best for adversarial training. Finally, we evaluate our approach on CIFAR-10 against ∞ and 2 norm-bounded perturbations of size = 8/255 and = 128/255, respectively. We show large absolute improvements of +2.93\% and +2.16\% in robust accuracy compared to previous state-of-the-art methods. In particular, against ∞ norm-bounded perturbations of size = 8/255, our model reaches 60.07\% robust accuracy without using any external data. We also achieve a signiﬁcant performance boost with this approach while using other architectures and datasets such as CIFAR-100, SVHN and TINYIMAGENET.},
	language = {en},
	author = {Rebufﬁ, Sylvestre-Alvise and Gowal, Sven and Calian, Dan and Stimberg, Florian and Wiles, Olivia and Mann, Timothy},
}

@misc{noauthor_httpsarxivorgpdf191006548_nodate,
	title = {https://arxiv.org/pdf/1910.06548},
	url = {https://arxiv.org/pdf/1910.06548},
	urldate = {2024-08-27},
}

@misc{poulos_training_2019,
	title = {Training {CNNs} faster with {Dynamic} {Input} and {Kernel} {Downsampling}},
	url = {http://arxiv.org/abs/1910.06548},
	abstract = {We reduce training time in convolutional networks (CNNs) with a method that, for some of the mini-batches: a) scales down the resolution of input images via downsampling, and b) reduces the forward pass operations via pooling on the convolution ﬁlters. Training is performed in an interleaved fashion; some batches undergo the regular forward and backpropagation passes with original network parameters, whereas others undergo a forward pass with pooled ﬁlters and downsampled inputs. Since pooling is differentiable, the gradients of the pooled ﬁlters propagate to the original network parameters for a standard parameter update. The latter phase requires fewer ﬂoating point operations and less storage due to the reduced spatial dimensions in feature maps and ﬁlters. The key idea is that this phase leads to smaller and approximate updates and thus slower learning, but at signiﬁcantly reduced cost, followed by passes that use the original network parameters as a reﬁnement stage. Deciding how often and for which batches the downsmapling occurs can be done either stochastically or deterministically, and can be deﬁned as a training hyperparameter itself. Experiments on residual architectures show that we can achieve up to 23\% reduction in training time with minimal loss in validation accuracy.},
	language = {en},
	urldate = {2024-08-27},
	publisher = {arXiv},
	author = {Poulos, Zissis and Nouri, Ali and Moshovos, Andreas},
	month = oct,
	year = {2019},
	note = {arXiv:1910.06548 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hohlfeld_classification_2022,
	title = {Classification of {Beer} {Bottles} using {Object} {Detection} and {Transfer} {Learning}},
	url = {http://arxiv.org/abs/2201.03791},
	doi = {10.48550/arXiv.2201.03791},
	abstract = {Classification problems are common in Computer Vision. Despite this, there is no dedicated work for the classification of beer bottles. As part of the challenge of the master course Deep Learning, a dataset of 5207 beer bottle images and brand labels was created. An image contains exactly one beer bottle. In this paper we present a deep learning model which classifies pictures of beer bottles in a two step approach. As the first step, a Faster-R-CNN detects image sections relevant for classification independently of the brand. In the second step, the relevant image sections are classified by a ResNet-18. The image section with the highest confidence is returned as class label. We propose a model, with which we surpass the classic one step transfer learning approach and reached an accuracy of 99.86\% during the challenge on the final test dataset. We were able to achieve 100\% accuracy after the challenge ended},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Hohlfeld, Philipp and Ostermeier, Tobias and Brandl, Dominik},
	month = jan,
	year = {2022},
	note = {arXiv:2201.03791 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{dong_supervised_2022,
	title = {Supervised learning-based retinal vascular segmentation by {M}-{UNet} full convolutional neural network},
	volume = {16},
	issn = {1863-1711},
	url = {https://doi.org/10.1007/s11760-022-02132-3},
	doi = {10.1007/s11760-022-02132-3},
	abstract = {The accurate vessel segmentation for retinal image is the most conducive to early diagnosis of various eye-related diseases. Most deep learning models are used to segment the vessel by green channel grey image and random sampling of whole image that lead to insufficient training samples and low accuracy probably. To address this issue, a novel retinal vessel segmentation model based on deep learning is proposed, which is called Multichannel U-Net (M-UNet) combined with multi-scale equalization sampling for extracting vessel networks. The multi-scale equalization sampling patches are used to splice an image as new data. And the M-UNet structure is utilized to train the retinal vessel segmentation model. The input of the model is the three channel image replacing the grey image. The DRIVE and STARE database are used to evaluate the performance. Experimental results indicate that the proposed method can obtain better vessel networks, especially for the extraction of peripheral vascular structure. The average accuracy of DRIVE is 0.9716, sensitivity is 0.8168, specificity is 0.9860, and Area Under Curve (AUC) is up to 0.9843. The performances are more competitive than state-of-the-art methods.},
	language = {en},
	number = {7},
	urldate = {2024-08-19},
	journal = {Signal, Image and Video Processing},
	author = {Dong, Heng and Zhang, Ting and Zhang, Tianyi and Wei, Lifang},
	month = oct,
	year = {2022},
	keywords = {Equalization sampling, Multichannel U-Net, Retinal image, Vessel segmentation},
	pages = {1755--1761},
}

@misc{choi_empirical_2020,
	title = {On {Empirical} {Comparisons} of {Optimizers} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/1910.05446},
	abstract = {Selecting an optimizer is a central step in the contemporary deep learning pipeline. In this paper, we demonstrate the sensitivity of optimizer comparisons to the hyperparameter tuning protocol. Our ﬁndings suggest that the hyperparameter search space may be the single most important factor explaining the rankings obtained by recent empirical comparisons in the literature. In fact, we show that these results can be contradicted when hyperparameter search spaces are changed. As tuning effort grows without bound, more general optimizers should never underperform the ones they can approximate (i.e., Adam should never perform worse than momentum), but recent attempts to compare optimizers either assume these inclusion relationships are not practically relevant or restrict the hyperparameters in ways that break the inclusions. In our experiments, we ﬁnd that inclusion relationships between optimizers matter in practice and always predict optimizer comparisons. In particular, we ﬁnd that the popular adaptive gradient methods never underperform momentum or gradient descent. We also report practical tips around tuning often ignored hyperparameters of adaptive gradient methods and raise concerns about fairly benchmarking optimizers for neural network training.},
	language = {en},
	urldate = {2024-08-19},
	publisher = {arXiv},
	author = {Choi, Dami and Shallue, Christopher J. and Nado, Zachary and Lee, Jaehoon and Maddison, Chris J. and Dahl, George E.},
	month = jun,
	year = {2020},
	note = {arXiv:1910.05446 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@phdthesis{bronder_optimisation_2023,
	type = {{doctoralThesis}},
	title = {Optimisation of mechanical metamaterials using machine learning - establishing a new workflow},
	copyright = {Namensnennung-NichtKommerziell-KeineBearbeitung 3.0 Deutschland},
	url = {https://publikationen.sulb.uni-saarland.de/handle/20.500.11880/36406},
	abstract = {Optimised design, either for components or materials, is a topic on the rise in our century. It is strongly linked with the thought of lightweight construction and resource management. Thus, the aim of this work is to provide a faster way to optimise the microstructure of mechanical metamaterials utilising machine learning. An optimisation scheme with neural networks is developed and experimentally validated on auxetic materials, with their governing property being a negative Poisson’s ratio. This method consists of multiple steps, beginning with finding a representative volume element, followed by an investigation of the design space, creation of a database to train a neural network and finalised by an optimisation of the structure. Each step is validated by experiments and corrections to the model are applied if necessary. For the auxetics the aim is to find the structure with the maximal mass specific energy absorption capacity and negative as possible Poisson’s ratio. The final optimisation routine is then transferred to another metamaterial structure, the pentamodes. Here, an investigation and successful optimisation for maximised damping and stiffness solely based on simulations is launched. In addition, adaptive sampling is applied in order to reduce the required number of simulations.},
	language = {en},
	urldate = {2024-08-19},
	school = {Saarländische Universitäts- und Landesbibliothek},
	author = {Bronder, Stefan},
	year = {2023},
	doi = {10.22028/D291-40518},
	note = {Accepted: 2023-09-08T12:38:45Z},
}

@phdthesis{muller_klassifizierung_2023,
	type = {{doctoralThesis}},
	title = {Klassifizierung komplexer {Gefüge} mit maschinellem {Lernen} am {Beispiel} bainitischer {Stähle}},
	url = {https://publikationen.sulb.uni-saarland.de/handle/20.500.11880/37526},
	abstract = {Ziel der vorliegenden Arbeit ist eine objektive und reproduzierbare Klassifizierung bainitischer Gefüge in Zweiphasen- und Multiphasenstählen mit Methoden des maschinellen Lernens (ML). Aufgrund der Komplexität der bainitischen Gefüge stellt die Zuordnung der für das ML benötigten Grundwahrheit eine enorme Herausforderung dar. Zur Implementierung der ML-basierten Auswertungen wird daher ein ganzheitlicher, materialwissenschaftlich basierter Ansatz vorgeschlagen, der sich nicht lediglich auf ML-Algorithmen beschränkt, sondern auch alle Schritte hin zum Erhalt einer Gefügeaufnahme berücksichtigt. In diesem Rahmen wird eine Methodik zur korrelativen Charakterisierung mittels Lichtmikroskopie (LM), Rasterelektronenmikroskopie (REM) und Elektronenrückstreubeugung (EBSD) entwickelt, die eine kombinierte Gefügequantifizierung aus Mikroskopaufnahmen und den EBSD-Daten erlaubt und darüber überhaupt erst die objektive und reproduzierbare Zuordnung der Grundwahrheit zulässt.},
	language = {de},
	urldate = {2024-08-19},
	school = {Saarländische Universitäts- und Landesbibliothek},
	author = {Müller, Martin},
	year = {2023},
	doi = {10.22028/D291-41897},
	note = {Accepted: 2024-04-23T10:03:24Z},
}

@inproceedings{ali_review_2022,
	title = {Review on {Image} {Segmentation} {Methods} {Using} {Deep} {Learning}},
	url = {https://ieeexplore.ieee.org/document/10075607/?arnumber=10075607},
	doi = {10.1109/ICOASE56293.2022.10075607},
	abstract = {In recent years, the machine learning field has been inundated with a variety of deep learning methods. Different deep learning model types, including recurrent neural networks (RNNs), convolutional neural networks (CNNs), adversarial neural networks (ANNs), and autoencoders, are successfully tackling challenging computer vision problems including image detection and segmentation in an unconstrained environment. Although image segmentation has received a lot of interest, there have been several new deep learning methods discovered with regard to object detection and recognition. An academic review of deep learning image segmentation methods is presented in this article. In this study, the major goal is to offer a sensible comprehension of the basic approaches that have already made a substantial contribution to the domain of image segmentation throughout the years. The article describes the existing state of image segmentation, and goes on to make the argument that deep learning has revolutionized this field. Afterwards, segmentation algorithms have been scientifically classified and optimized, each with their own special contribution. With a variety of informative narratives, the reader may be able to understand the internal workings of these processes more quickly.},
	urldate = {2024-08-19},
	booktitle = {2022 4th {International} {Conference} on {Advanced} {Science} and {Engineering} ({ICOASE})},
	author = {Ali, Nabeel N. and Kako, Najdavan A. and Abdi, Abdo Sulaiman},
	month = sep,
	year = {2022},
	keywords = {Benchmark testing, Clustering algorithms, Convolutional neural networks, Deep learning, Encoder-decoder models, Image segmentation, Object detection, Performance evaluation, Recurrent neural networks, Semantic segmentation},
	pages = {7--12},
}

@article{arulananth_semantic_2024,
	title = {Semantic segmentation of urban environments: {Leveraging} {U}-{Net} deep learning model for cityscape image analysis},
	volume = {19},
	issn = {1932-6203},
	shorttitle = {Semantic segmentation of urban environments},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0300767},
	doi = {10.1371/journal.pone.0300767},
	abstract = {Semantic segmentation of cityscapes via deep learning is an essential and game-changing research topic that offers a more nuanced comprehension of urban landscapes. Deep learning techniques tackle urban complexity and diversity, which unlocks a broad range of applications. These include urban planning, transportation management, autonomous driving, and smart city efforts. Through rich context and insights, semantic segmentation helps decision-makers and stakeholders make educated decisions for sustainable and effective urban development. This study investigates an in-depth exploration of cityscape image segmentation using the U-Net deep learning model. The proposed U-Net architecture comprises an encoder and decoder structure. The encoder uses convolutional layers and down sampling to extract hierarchical information from input images. Each down sample step reduces spatial dimensions, and increases feature depth, aiding context acquisition. Batch normalization and dropout layers stabilize models and prevent overfitting during encoding. The decoder reconstructs higher-resolution feature maps using "UpSampling2D" layers. Through extensive experimentation and evaluation of the Cityscapes dataset, this study demonstrates the effectiveness of the U-Net model in achieving state-of-the-art results in image segmentation. The results clearly shown that, the proposed model has high accuracy, mean IOU and mean DICE compared to existing models.},
	language = {en},
	number = {4},
	urldate = {2024-08-19},
	journal = {PLOS ONE},
	author = {Arulananth, T. S. and Kuppusamy, P. G. and Ayyasamy, Ramesh Kumar and Alhashmi, Saadat M. and Mahalakshmi, M. and Vasanth, K. and Chinnasamy, P.},
	month = may,
	year = {2024},
	note = {Publisher: Public Library of Science},
	keywords = {Deep learning, Human mobility, Imaging techniques, Machine learning, Neural networks, Preprocessing, Transportation, Urban environments},
	pages = {e0300767},
}

@article{williams_unied_nodate,
	title = {A {Uniﬁed} {Framework} for {U}-{Net} {Design} and {Analysis}},
	abstract = {U-Nets are a go-to neural architecture across numerous tasks for continuous signals on a square such as images and Partial Differential Equations (PDE), however their design and architecture is understudied. In this paper, we provide a framework for designing and analysing general U-Net architectures. We present theoretical results which characterise the role of the encoder and decoder in a U-Net, their high-resolution scaling limits and their conjugacy to ResNets via preconditioning. We propose Multi-ResNets, U-Nets with a simpliﬁed, wavelet-based encoder without learnable parameters. Further, we show how to design novel U-Net architectures which encode function constraints, natural bases, or the geometry of the data. In diffusion models, our framework enables us to identify that highfrequency information is dominated by noise exponentially faster, and show how U-Nets with average pooling exploit this. In our experiments, we demonstrate how Multi-ResNets achieve competitive and often superior performance compared to classical U-Nets in image segmentation, PDE surrogate modelling, and generative modelling with diffusion models. Our U-Net framework paves the way to study the theoretical properties of U-Nets and design natural, scalable neural architectures for a multitude of problems beyond the square.},
	language = {en},
	author = {Williams, Christopher and Falck, Fabian and Deligiannidis, George and Holmes, Chris and Doucet, Arnaud and Syed, Saifuddin},
}

@article{di_benedetto_u-net-based_2023,
	title = {U-{Net}-{Based} {CNN} {Architecture} for {Road} {Crack} {Segmentation}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2412-3811},
	url = {https://www.mdpi.com/2412-3811/8/5/90},
	doi = {10.3390/infrastructures8050090},
	abstract = {Many studies on the semantic segmentation of cracks using the machine learning (ML) technique can be found in the relevant literature. To date, the results obtained are quite good, but often the accuracy of the trained model and the results obtained are evaluated using traditional metrics only, and in most cases, the goal is to detect only the occurrence of cracks. Particular attention should be paid to the thickness of the segmented crack since, in road pavement maintenance, the width of the crack is the main parameter and is the one that characterizes the severity levels. The aim of our study is to optimize the crack segmentation process through the implementation of a modified U-Net model-based algorithm. For this, the Crack500 dataset is used, and then the results are compared with those obtained from the U-Net algorithm, which is currently found to be the most accurate and performant in the literature. The results are promising and accurate, as the findings on the shape and width of the segmented cracks are very close to reality.},
	language = {en},
	number = {5},
	urldate = {2024-08-19},
	journal = {Infrastructures},
	author = {Di Benedetto, Alessandro and Fiani, Margherita and Gujski, Lucas Matias},
	month = may,
	year = {2023},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {CNN, U-net, computer vision, deep learning, road crack},
	pages = {90},
}

@article{shao_application_2023,
	title = {Application of {U}-{Net} and {Optimized} {Clustering} in {Medical} {Image} {Segmentation}: {A} {Review}},
	volume = {136},
	issn = {1526-1506},
	shorttitle = {Application of {U}-{Net} and {Optimized} {Clustering} in {Medical} {Image} {Segmentation}},
	url = {https://www.techscience.com/CMES/v136n3/51829},
	doi = {10.32604/cmes.2023.025499},
	abstract = {As a mainstream research direction in the field of image segmentation, medical image segmentation plays a key role in the quantification of lesions, three-dimensional reconstruction, region of interest extraction and so on. Compared with natural images, medical images have a variety of modes. Besides, the emphasis of information which is conveyed by images of different modes is quite different. Because it is time-consuming and inefficient to manually segment medical images only by professional and experienced doctors. Therefore, large quantities of automated medical image segmentation methods have been developed. However, until now, researchers have not developed a universal method for all types of medical image segmentation. This paper reviews the literature on segmentation techniques that have produced major breakthroughs in recent years. Among the large quantities of medical image segmentation methods, this paper mainly discusses two categories of medical image segmentation methods. One is the improved strategies based on traditional clustering method. The other is the research progress of the improved image segmentation network structure model based on U-Net. The power of technology proves that the performance of the deep learning-based method is significantly better than that of the traditional method. This paper discussed both advantages and disadvantages of different algorithms and detailed how these methods can be used for the segmentation of lesions or other organs and tissues, as well as possible technical trends for future work.},
	language = {en},
	number = {3},
	urldate = {2024-08-19},
	journal = {Computer Modeling in Engineering \& Sciences},
	author = {Shao, Jiaqi and Chen, Shuwen and Zhou, Jin and Zhu, Huisheng and Wang, Ziyi and Brown, Mackenzie},
	year = {2023},
	pages = {2173--2219},
}

@article{siami_semantic_2024,
	title = {Semantic segmentation of thermal defects in belt conveyor idlers using thermal image augmentation and {U}-{Net}-based convolutional neural networks},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-55864-2},
	doi = {10.1038/s41598-024-55864-2},
	abstract = {The belt conveyor (BC) is the main means of horizontal transportation of bulk materials at mining sites. The sudden fault in BC modules may cause unexpected stops in production lines. With the increasing number of applications of inspection mobile robots in condition monitoring (CM) of industrial infrastructure in hazardous environments, in this article we introduce an image processing pipeline for automatic segmentation of thermal defects in thermal images captured from BC idlers using a mobile robot. This study follows the fact that CM of idler temperature is an important task for preventing sudden breakdowns in BC system networks. We compared the performance of three different types of U-Net-based convolutional neural network architectures for the identification of thermal anomalies using a small number of hand-labeled thermal images. Experiments on the test data set showed that the attention residual U-Net with binary cross entropy as the loss function handled the semantic segmentation problem better than our previous research and other studied U-Net variations.},
	language = {en},
	number = {1},
	urldate = {2024-08-19},
	journal = {Scientific Reports},
	author = {Siami, Mohammad and Barszcz, Tomasz and Wodecki, Jacek and Zimroz, Radoslaw},
	month = mar,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Electrical and electronic engineering, Engineering},
	pages = {5748},
}

@article{yuan_-net_2023,
	title = {μ-{Net}: {Medical} image segmentation using efficient and effective deep supervision},
	volume = {160},
	issn = {0010-4825},
	shorttitle = {μ{\textless}math{\textgreater}{\textless}mi is="true"{\textgreater}μ{\textless}/mi{\textgreater}{\textless}/math{\textgreater}-{Net}},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482523004286},
	doi = {10.1016/j.compbiomed.2023.106963},
	abstract = {Although the existing deep supervised solutions have achieved some great successes in medical image segmentation, they have the following shortcomings; (i) semantic difference problem: since they are obtained by very different convolution or deconvolution processes, the intermediate masks and predictions in deep supervised baselines usually contain semantics with different depth, which thus hinders the models’ learning capabilities; (ii) low learning efficiency problem: additional supervision signals will inevitably make the training of the models more time-consuming. Therefore, in this work, we first propose two deep supervised learning strategies, U-Net-Deep and U-Net-Auto, to overcome the semantic difference problem. Then, to resolve the low learning efficiency problem, upon the above two strategies, we further propose a new deep supervised segmentation model, called μ-Net, to achieve not only effective but also efficient deep supervised medical image segmentation by introducing a tied-weight decoder to generate pseudo-labels with more diverse information and also speed up the convergence in training. Finally, three different types of μ-Net-based deep supervision strategies are explored and a Similarity Principle of Deep Supervision is further derived to guide future research in deep supervised learning. Experimental studies on four public benchmark datasets show that μ-Net greatly outperforms all the state-of-the-art baselines, including the state-of-the-art deeply supervised segmentation models, in terms of both effectiveness and efficiency. Ablation studies sufficiently prove the soundness of the proposed Similarity Principle of Deep Supervision, the necessity and effectiveness of the tied-weight decoder, and using both the segmentation and reconstruction pseudo-labels for deep supervised learning.},
	urldate = {2024-08-19},
	journal = {Computers in Biology and Medicine},
	author = {Yuan, Di and Xu, Zhenghua and Tian, Biao and Wang, Hening and Zhan, Yuefu and Lukasiewicz, Thomas},
	month = jun,
	year = {2023},
	keywords = {Deep supervised learning, Medical image segmentation, Similarity principle of deep supervision, Tied-weight decoder},
	pages = {106963},
}

@inproceedings{enshaei_automated_2020,
	title = {Automated detection of textured-surface defects using {UNet}-based semantic segmentation network},
	url = {https://ieeexplore.ieee.org/abstract/document/9187023},
	doi = {10.1109/ICPHM49022.2020.9187023},
	abstract = {Over the recent years, developing a reliable auto-mated visual inspection system/approach for manufacturing and industry sectors which are moving toward smart manufacturing operations faces lots of significant challenges. Traditional visual inspection techniques which are developed based on manually extracted features, can rarely be generalized and have shown weak performance in real applications in different industries. In this paper, we propose a novel and automated visual inspection system which can outperform the statistical methods in terms of detection and the quantification of anomalies in image data for performing critical industrial tasks such as detecting micro scratches on product. In particular, an end-to-end UNet-based fully convolutional neural network for automated defect detection in industrial surfaces is designed and developed. The proposed network has the capability to accept raw images as input and the output is pixel-wise masks. In order to avoid overfitting and improve the model generalization, we use real-time data augmentation approach during our training phase. To evaluate the performance of the proposed model, we use a publicly available data set containing ten different types of textured-surfaces with their associated weakly annotated masks. The findings indicate that despite working with roughly annotated labels, our results are in agreement with previous works and show improvements regarding the detection time.},
	urldate = {2024-08-12},
	booktitle = {2020 {IEEE} {International} {Conference} on {Prognostics} and {Health} {Management} ({ICPHM})},
	author = {Enshaei, Nastaran and Ahmad, Safwan and Naderkhani, Farnoosh},
	month = jun,
	year = {2020},
	keywords = {Data models, Feature extraction, Inspection, Prognostics and health management, Surface texture, Training, Visualization},
	pages = {1--5},
}

@misc{oktay_attention_2018,
	title = {Attention {U}-{Net}: {Learning} {Where} to {Look} for the {Pancreas}},
	shorttitle = {Attention {U}-{Net}},
	url = {http://arxiv.org/abs/1804.03999},
	doi = {10.48550/arXiv.1804.03999},
	abstract = {We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y. and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
	month = may,
	year = {2018},
	note = {arXiv:1804.03999 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{yasmin_concrete_2024,
	title = {Concrete spalling detection system based on semantic segmentation using deep architectures},
	volume = {300},
	issn = {0045-7949},
	url = {https://www.sciencedirect.com/science/article/pii/S0045794924001275},
	doi = {10.1016/j.compstruc.2024.107398},
	abstract = {This paper presents a method for detecting the location of spalling and assessing the severity level of the spalling in concrete surfaces. The proposed method is constructed based on deep learning architectures and multi-class semantic segmentation. The proposed method can detect each pixel as a non-spalling, a deep-spalling, or a shallow-spalling. The proposed method consists of three different deep learning architectures with several encoders as backbone networks. Both qualitative and quantitative analyses show that the deep learning architecture with a certain encoder network can detect spalling with different severity levels very well. Additionally, the paper proposes a method to analyze the deep spalling areas of concrete to show their severity levels. The performance analysis shows that this approach provides very convincing results with respect to the actual affected spalling areas. The results convey that this paper achieved a higher level of performance for detecting spalling and assessing the severity of the spalling.},
	urldate = {2024-08-12},
	journal = {Computers \& Structures},
	author = {Yasmin, Tamanna and La, Duc and La, Kien and Nguyen, Minh Tuan and La, Hung Manh},
	month = aug,
	year = {2024},
	keywords = {Deep architecture, Deep neural networks, Encoder-decoder, Spalling detection, Spalling severity},
	pages = {107398},
}

@article{arafin_deep_2024,
	title = {Deep learning-based concrete defects classification and detection using semantic segmentation},
	volume = {23},
	issn = {1475-9217},
	url = {https://doi.org/10.1177/14759217231168212},
	doi = {10.1177/14759217231168212},
	abstract = {Visual damage detection of infrastructure using deep learning (DL)-based computational approaches can facilitate a potential solution to reduce subjectivity yet increase the accuracy of the damage diagnoses and accessibility in a structural health monitoring (SHM) system. However, despite remarkable advances with DL-based SHM, the most significant challenges to achieving the real-time implication are the limited available defects image databases and the selection of DL networks depth. To address these challenges, this research has created a diverse dataset with concrete crack (4087) and spalling (1100) images and used it for damage condition assessment by applying convolutional neural network (CNN) algorithms. CNN-classifier models are used to identify different types of defects and semantic segmentation for labeling the defect patterns within an image. Three CNN-based models—Visual Geometry Group (VGG)19, ResNet50, and InceptionV3 are incorporated as CNN-classifiers. For semantic segmentation, two encoder-decoder models, U-Net and pyramid scene parsing network architecture are developed based on four backbone models, including VGG19, ResNet50, InceptionV3, and EfficientNetB3. The CNN-classifier models are analyzed on two optimizers—stochastic gradient descent (SGD), root mean square propagation (RMSprop), and learning rates—0.1, 0.001, and 0.0001. However, the CNN-segmentation models are analyzed for SGD and adaptive moment estimation, trained with three different learning rates—0.1, 0.01, and 0.0001, and evaluated based on accuracy, intersection over union, precision, recall, and F1-score. InceptionV3 achieves the best performance for defects classification with an accuracy of 91.98\% using the RMSprop optimizer. For crack segmentation, EfficientNetB3-based U-Net, and for spalling segmentation, IncenptionV3-based U-Net model outperformed all other algorithms, with an F1-score of 95.66 and 89.43\%, respectively.},
	language = {en},
	number = {1},
	urldate = {2024-08-12},
	journal = {Structural Health Monitoring},
	author = {Arafin, Palisa and Billah, AHM Muntasir and Issa, Anas},
	month = jan,
	year = {2024},
	note = {Publisher: SAGE Publications},
	pages = {383--409},
}

@article{zschech_mit_2021,
	title = {Mit {Computer} {Vision} zur automatisierten {Qualitätssicherung} in der industriellen {Fertigung}: {Eine} {Fallstudie} zur {Klassifizierung} von {Fehlern} in {Solarzellen} mittels {Elektrolumineszenz}-{Bildern}},
	volume = {58},
	issn = {2198-2775},
	shorttitle = {Mit {Computer} {Vision} zur automatisierten {Qualitätssicherung} in der industriellen {Fertigung}},
	url = {https://doi.org/10.1365/s40702-020-00641-8},
	doi = {10.1365/s40702-020-00641-8},
	abstract = {Die Qualitätssicherung bei der Produktion von Solarzellen ist ein entscheidender Faktor, um langfristige Leistungsgarantien auf Solarpanels gewähren zu können. Die vorliegende Arbeit leistet hierzu einen Beitrag zur automatisierten Fehlererkennung auf Wafern, indem Elektrolumineszenz-Bilder eines realen Herstellungsszenarios mithilfe von verschiedenen Computer-Vision-Modellen klassifiziert werden. Die Herausforderung besteht hierbei nicht nur darin, defekte Wafer von funktionsfähigen zu separieren, sondern gleichzeitig auch zwischen spezifischen Fehlerarten zu unterscheiden, während geringe Inferenzzeiten sicherzustellen sind. Zu diesem Zweck werden neben einfachen statistischen Modellen verschiedene Deep-Learning-Architekturen auf Basis von Convolutional Neural Networks (CNNs) verprobt und miteinander vergleichen. Ziel der Arbeit ist es, verschiedene Klassifizierungsansätze unterschiedlicher Komplexität zu testen und auf ihre praktische Einsatzfähigkeit unter realen Bedingungen zu untersuchen. Die Fallstudie zeigt, dass je nach Situation unterschiedliche Modelle ihre Existenzberechtigung haben und in Kombination sehr gute Ergebnisse erzielen. So lassen sich bereits mit statistischen Modellen und einfachen CNN-Varianten zuverlässige Aussagen mit Genauigkeiten von über 99 \% bei Fehlertypen einfacher bis mittlerer Erkennbarkeit realisieren. Werden die Fehlerbilder demgegenüber diffuser und soll die Nachvollziehbarkeit der Ergebnisse durch positionsgenaue Lokalisierung von Fehlerobjekten gewährleistet werden, sind fortgeschrittenere Ansätze auf Basis sogenannter Region-Proposal-Netzwerke erforderlich, die allerdings auch mit einem erhöhten Labeling-Aufwand beim Annotieren der Fehlerobjekte einhergehen. Da die Umsetzung sämtlicher Modelle ausschließlich auf Open Source Tools wie zum Beispiel TensorFlow, Keras und OpenCV basiert, demonstriert die Fallstudie zudem, welche Möglichkeiten durch frei verfügbare Lösungen im Bereich von Computer Vision geboten werden.},
	language = {de},
	number = {2},
	urldate = {2024-08-12},
	journal = {HMD Praxis der Wirtschaftsinformatik},
	author = {Zschech, Patrick and Sager, Christoph and Siebers, Philipp and Pertermann, Maik},
	month = apr,
	year = {2021},
	keywords = {Computer vision, Deep Learning, Deep learning, Industrie, Industry, Maschinelle Bildverarbeitung, Object detection, Objekterkennung, Photovoltaic, Photovoltaik, Quality assurance, Qualitätssicherung},
	pages = {321--342},
}

@misc{mwiti_image_2022,
	title = {Image {Segmentation}: {Architectures}, {Losses}, {Datasets}, and {Frameworks}},
	shorttitle = {Image {Segmentation}},
	url = {https://neptune.ai/blog/image-segmentation},
	abstract = {Comprehensive analysis of image segmentation: architectures, loss functions, datasets, and frameworks in modern applications.},
	language = {en-US},
	urldate = {2024-08-12},
	journal = {neptune.ai},
	author = {Mwiti, Derrick},
	month = jul,
	year = {2022},
}

@misc{noauthor_u-net_2023,
	title = {U-{Net} {Architecture} {Explained}},
	url = {https://www.geeksforgeeks.org/u-net-architecture-explained/},
	abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
	language = {en-US},
	urldate = {2024-08-12},
	journal = {GeeksforGeeks},
	month = jun,
	year = {2023},
	note = {Section: AI-ML-DS},
}

@misc{krombach_bildklassifizierung_2023,
	title = {Bildklassifizierung mit {PyTorch} – {So} funktioniert's},
	url = {https://proki-hannover.de/bildklassifizierung-mit-pytorch-so-funktionierts/},
	abstract = {In diesem Beitrag zeigen wir, wie Sie einen Bildklassifizierer trainieren. Lesen Sie jetzt unsere Schritt-für-Schritt-Anleitung.},
	language = {de-DE},
	urldate = {2024-08-12},
	journal = {ProKI-Hannover},
	author = {Krombach, Paul},
	month = jul,
	year = {2023},
}

@misc{klingler_image_2023,
	title = {Image {Segmentation} with {Deep} {Learning} ({Guide})},
	url = {https://viso.ai/deep-learning/image-segmentation-using-deep-learning/},
	abstract = {Learn about image segmentation with deep learning and the most important datasets. Find the most popular applications of image segmentation.},
	language = {en-US},
	urldate = {2024-08-12},
	journal = {viso.ai},
	author = {Klingler, Nico},
	month = sep,
	year = {2023},
}

@misc{potter_use_2021,
	title = {Use cases of {Image} {Segmentation} {Using} {Deep} {Learning}},
	url = {https://becominghuman.ai/use-cases-of-image-segmentation-using-deep-learning-90a40f1a1d97},
	abstract = {In the last decade, computer vision technology has advanced substantially, thanks to advances in AI and deep learning methodologies. It is…},
	language = {en},
	urldate = {2024-08-12},
	journal = {Medium},
	author = {Potter, Rayan},
	month = dec,
	year = {2021},
}

@misc{potter_why_2021,
	title = {Why {Image} {Segmentation} is {Needed}: {Image} {Segmentation} {Techniques}},
	shorttitle = {Why {Image} {Segmentation} is {Needed}},
	url = {https://becominghuman.ai/why-image-segmentation-is-needed-image-segmentation-techniques-ee52b92e651a},
	abstract = {In computer vision world, objects can be viewed through images. And classifying, tagging, segmenting and annotating these are images are…},
	language = {en},
	urldate = {2024-08-12},
	journal = {Medium},
	author = {Potter, Rayan},
	month = jan,
	year = {2021},
}

@misc{noauthor_image_nodate,
	title = {Image segmentation detailed overview [{Updated} 2024] {\textbar} {SuperAnnotate}},
	url = {https://www.superannotate.com/blog/image-segmentation-for-machine-learning},
	abstract = {Discover image segmentation types, techniques, and applications. Leverage your image segmentation project by reading this detailed overview.},
	language = {en},
	urldate = {2024-08-12},
}

@article{valente_developments_2023,
	title = {Developments in {Image} {Processing} {Using} {Deep} {Learning} and {Reinforcement} {Learning}},
	volume = {9},
	issn = {2313-433X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10607786/},
	doi = {10.3390/jimaging9100207},
	abstract = {The growth in the volume of data generated, consumed, and stored, which is estimated to exceed 180 zettabytes in 2025, represents a major challenge both for organizations and for society in general. In addition to being larger, datasets are increasingly complex, bringing new theoretical and computational challenges. Alongside this evolution, data science tools have exploded in popularity over the past two decades due to their myriad of applications when dealing with complex data, their high accuracy, flexible customization, and excellent adaptability. When it comes to images, data analysis presents additional challenges because as the quality of an image increases, which is desirable, so does the volume of data to be processed. Although classic machine learning (ML) techniques are still widely used in different research fields and industries, there has been great interest from the scientific community in the development of new artificial intelligence (AI) techniques. The resurgence of neural networks has boosted remarkable advances in areas such as the understanding and processing of images. In this study, we conducted a comprehensive survey regarding advances in AI design and the optimization solutions proposed to deal with image processing challenges. Despite the good results that have been achieved, there are still many challenges to face in this field of study. In this work, we discuss the main and more recent improvements, applications, and developments when targeting image processing applications, and we propose future research directions in this field of constant and fast evolution.},
	number = {10},
	urldate = {2024-08-12},
	journal = {Journal of Imaging},
	author = {Valente, Jorge and António, João and Mora, Carlos and Jardim, Sandra},
	month = sep,
	year = {2023},
	pmid = {37888314},
	pmcid = {PMC10607786},
	pages = {207},
}

@article{li_semi-supervised_2023,
	title = {Semi-{Supervised} {Remote} {Sensing} {Image} {Semantic} {Segmentation} {Method} {Based} on {Deep} {Learning}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/12/2/348},
	doi = {10.3390/electronics12020348},
	abstract = {In this paper, we study the semi-supervised semantic segmentation problem via limited labeled samples and a large number of unlabeled samples. We propose a self-learning semi-supervised approach for the semantic segmentation of high-resolution remote sensing images. Our approach uses two networks (UNet and DeepLabV3) to predict the labels of the same unlabeled sample, and the pseudo labels samples with high prediction consistency are added to the training samples to improve the accuracy of semantic segmentation under the condition of limited labeled samples. Our method expands training data samples by using unlabeled data samples with pseudo labels. In order to verify the effectiveness of the proposed method, some experiments were conducted on the improved ISPRS Vaihingen 2D Semantic Labeling dataset using the method that we proposed. We focus on the extraction of forest and vegetation information and focus on the impact of a large number of unlabeled samples on the precision of semantic segmentation, we combine water, surface, buildings, cars, and background into one category and named others, and we call the changed dataset the improved ISPRS Vaihingen dataset. The experimental results show that the proposed method can effectively improve the semantic segmentation accuracy of high-scoring remote sensing images with limited samples than common deep semi-supervised learning.},
	language = {en},
	number = {2},
	urldate = {2024-08-12},
	journal = {Electronics},
	author = {Li, Linhui and Zhang, Wenjun and Zhang, Xiaoyan and Emam, Mahmoud and Jing, Weipeng},
	month = jan,
	year = {2023},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {convolutional neural network, high-resolution remote sensing images, semantic segmentation, semi-supervised},
	pages = {348},
}

@article{zhang_semi-supervised_2023,
	title = {Semi-{Supervised} {Semantic} {Segmentation}-{Based} {Remote} {Sensing} {Identification} {Method} for {Winter} {Wheat} {Planting} {Area} {Extraction}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-4395},
	url = {https://www.mdpi.com/2073-4395/13/12/2868},
	doi = {10.3390/agronomy13122868},
	abstract = {To address the cost issue associated with pixel-level image annotation in fully supervised semantic segmentation, a method based on semi-supervised semantic segmentation is proposed for extracting winter wheat planting areas. This approach utilizes self-training with pseudo-labels to learn from a small set of images with pixel-level annotations and a large set of unlabeled images, thereby achieving the extraction. In the constructed initial dataset, a random sampling strategy is employed to select 1/16, 1/8, 1/4, and 1/2 proportions of labeled data. Furthermore, in conjunction with the concept of consistency regularization, strong data augmentation techniques are applied to the unlabeled images, surpassing classical methods such as cropping and rotation to construct a semi-supervised model. This effectively alleviates overfitting caused by noisy labels. By comparing the prediction results of different proportions of labeled data using SegNet, DeepLabv3+, and U-Net, it is determined that the U-Net network model yields the best extraction performance. Moreover, the evaluation metrics MPA and MIoU demonstrate varying degrees of improvement for semi-supervised semantic segmentation compared to fully supervised semantic segmentation. Notably, the U-Net model trained with 1/16 labeled data outperforms the models trained with 1/8, 1/4, and 1/2 labeled data, achieving MPA and MIoU scores of 81.63\%, 73.31\%, 82.50\%, and 76.01\%, respectively. This method provides valuable insights for extracting winter wheat planting areas in scenarios with limited labeled data.},
	language = {en},
	number = {12},
	urldate = {2024-08-12},
	journal = {Agronomy},
	author = {Zhang, Mingmei and Xue, Yongan and Zhan, Yuanyuan and Zhao, Jinling},
	month = dec,
	year = {2023},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {data augmentation, self-training, sematic segmentation, semi-supervised classification, winter wheat},
	pages = {2868},
}

@article{jiang_retinal_2018,
	title = {Retinal blood vessel segmentation using fully convolutional network with transfer learning},
	volume = {68},
	issn = {0895-6111},
	url = {https://www.sciencedirect.com/science/article/pii/S0895611118302313},
	doi = {10.1016/j.compmedimag.2018.04.005},
	abstract = {Since the retinal blood vessel has been acknowledged as an indispensable element in both ophthalmological and cardiovascular disease diagnosis, the accurate segmentation of the retinal vessel tree has become the prerequisite step for automated or computer-aided diagnosis systems. In this paper, a supervised method is presented based on a pre-trained fully convolutional network through transfer learning. This proposed method has simplified the typical retinal vessel segmentation problem from full-size image segmentation to regional vessel element recognition and result merging. Meanwhile, additional unsupervised image post-processing techniques are applied to this proposed method so as to refine the final result. Extensive experiments have been conducted on DRIVE, STARE, CHASE\_DB1 and HRF databases, and the accuracy of the cross-database test on these four databases is state-of-the-art, which also presents the high robustness of the proposed approach. This successful result has not only contributed to the area of automated retinal blood vessel segmentation but also supports the effectiveness of transfer learning when applying deep learning technique to medical imaging.},
	urldate = {2024-08-12},
	journal = {Computerized Medical Imaging and Graphics},
	author = {Jiang, Zhexin and Zhang, Hao and Wang, Yi and Ko, Seok-Bum},
	month = sep,
	year = {2018},
	keywords = {Deep learning, Fully convolutional network, Pre-trained model, Retinal blood vessel segmentation, Transfer learning},
	pages = {1--15},
}

@misc{wang_fully_2022,
	title = {Fully {Self}-{Supervised} {Learning} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/2202.11981},
	doi = {10.48550/arXiv.2202.11981},
	abstract = {In this work, we present a fully self-supervised framework for semantic segmentation(FS{\textasciicircum}4). A fully bootstrapped strategy for semantic segmentation, which saves efforts for the huge amount of annotation, is crucial for building customized models from end-to-end for open-world domains. This application is eagerly needed in realistic scenarios. Even though recent self-supervised semantic segmentation methods have gained great progress, these works however heavily depend on the fully-supervised pretrained model and make it impossible a fully self-supervised pipeline. To solve this problem, we proposed a bootstrapped training scheme for semantic segmentation, which fully leveraged the global semantic knowledge for self-supervision with our proposed PGG strategy and CAE module. In particular, we perform pixel clustering and assignments for segmentation supervision. Preventing it from clustering a mess, we proposed 1) a pyramid-global-guided (PGG) training strategy to supervise the learning with pyramid image/patch-level pseudo labels, which are generated by grouping the unsupervised features. The stable global and pyramid semantic pseudo labels can prevent the segmentation from learning too many clutter regions or degrading to one background region; 2) in addition, we proposed context-aware embedding (CAE) module to generate global feature embedding in view of its neighbors close both in space and appearance in a non-trivial way. We evaluate our method on the large-scale COCO-Stuff dataset and achieved 7.19 mIoU improvements on both things and stuff objects},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Wang, Yuan and Zhuo, Wei and Li, Yucong and Wang, Zhi and Ju, Qi and Zhu, Wenwu},
	month = feb,
	year = {2022},
	note = {arXiv:2202.11981 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{zhao_interaction_2024,
	title = {Interaction semantic segmentation network via progressive supervised learning},
	volume = {35},
	doi = {10.1007/s00138-023-01500-4},
	abstract = {Semantic segmentation requires both low-level details and high-level semantics, without losing too much detail and ensuring the speed of inference. Most existing segmentation approaches leverage low- and high-level features from pre-trained models. We propose an interaction semantic segmentation network via Progressive Supervised Learning (ISSNet). Unlike a simple fusion of two sets of features, we introduce an information interaction module to embed semantics into image details, they jointly guide the response of features in an interactive way. We develop a simple yet effective boundary refinement module to provide refined boundary features for matching corresponding semantic. We introduce a progressive supervised learning strategy throughout the training level to significantly promote network performance, not architecture level. Our proposed ISSNet shows optimal inference time. We perform extensive experiments on four datasets, including Cityscapes, HazeCityscapes, RainCityscapes and CamVid. In addition to performing better in fine weather, proposed ISSNet also performs well on rainy and foggy days. We also conduct ablation study to demonstrate the role of our proposed component. Code is available at: https://github.com/Ruini94/ISSNet},
	journal = {Machine Vision and Applications},
	author = {Zhao, Ruini and Xie, Meilin and Feng, Xubin and Guo, Min and Su, Xiuqin and Zhang, Ping},
	month = feb,
	year = {2024},
}

@inproceedings{yuan_simple_2021,
	title = {A {Simple} {Baseline} for {Semi}-{Supervised} {Semantic} {Segmentation} {With} {Strong} {Data} {Augmentation}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Yuan_A_Simple_Baseline_for_Semi-Supervised_Semantic_Segmentation_With_Strong_Data_ICCV_2021_paper.html},
	language = {en},
	urldate = {2024-08-12},
	author = {Yuan, Jianlong and Liu, Yifan and Shen, Chunhua and Wang, Zhibin and Li, Hao},
	year = {2021},
	pages = {8229--8238},
}

@inproceedings{zhao_augmentation_2023,
	title = {Augmentation {Matters}: {A} {Simple}-{Yet}-{Effective} {Approach} to {Semi}-{Supervised} {Semantic} {Segmentation}},
	shorttitle = {Augmentation {Matters}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Augmentation_Matters_A_Simple-Yet-Effective_Approach_to_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.html},
	language = {en},
	urldate = {2024-08-12},
	author = {Zhao, Zhen and Yang, Lihe and Long, Sifan and Pi, Jimin and Zhou, Luping and Wang, Jingdong},
	year = {2023},
	pages = {11350--11359},
}

@article{van_engelen_survey_2020,
	title = {A survey on semi-supervised learning},
	volume = {109},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-019-05855-6},
	doi = {10.1007/s10994-019-05855-6},
	abstract = {Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption.},
	language = {en},
	number = {2},
	urldate = {2024-08-12},
	journal = {Machine Learning},
	author = {van Engelen, Jesper E. and Hoos, Holger H.},
	month = feb,
	year = {2020},
	keywords = {Classification, Machine learning, Semi-supervised learning},
	pages = {373--440},
}

@article{wang_attention-based_2022,
	title = {Attention-based deep learning for chip-surface-defect detection},
	volume = {121},
	issn = {1433-3015},
	url = {https://doi.org/10.1007/s00170-022-09425-4},
	doi = {10.1007/s00170-022-09425-4},
	abstract = {Unlike objects (such as cats and dogs) in the ImageNet, the surface defects on chips have a relatively tiny defect areas, yet they contain a large amount of information. The traditional deep learning methods have unsatisfied performance for tiny defects. Therefore, we proposed an object detection network combining attention with YOLOV4 for tiny defect detection, denoted as YOLOV4-SA. The network consists of a feature extraction backbone, a spatial attention module (SAM) and a feature fusion module. The SAM can correct the value of the feature map and highlight the defect areas, which identifies the tiny defects more effectively. To support current and future research, we also constructed a chip-surface-defect dataset, including a real set and a synthetic set. The real set contains non-defective and defective images. The synthetic set was generated by three new defect-generation methods. To the best of our knowledge, this is the first defect dataset in the field of advanced packaging chips. YOLOV4-SA was trained and tested on this dataset. Compared with classical defect detection methods, YOLOV4-SA achieved 78.47 mAP (mean of average precision), which was at least 7\% better than other methods. For tiny ink and crack defects, the mAP was improved by 11.92 compared with YOLOV4. Lastly, we also proposed a cascading deployment method which may be useful for industrial applications.},
	language = {en},
	number = {3},
	urldate = {2024-08-12},
	journal = {The International Journal of Advanced Manufacturing Technology},
	author = {Wang, Shuo and Wang, Hongyu and Yang, Fan and Liu, Fei and Zeng, Long},
	month = jul,
	year = {2022},
	keywords = {Attention Mechanism, Deep Learning, Defect Inspection, Surface Inspection, YOLOV4-SA},
	pages = {1957--1971},
}

@article{jiao_learning_2024,
	title = {Learning with limited annotations: {A} survey on deep semi-supervised learning for medical image segmentation},
	volume = {169},
	issn = {0010-4825},
	shorttitle = {Learning with limited annotations},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482523013057},
	doi = {10.1016/j.compbiomed.2023.107840},
	abstract = {Medical image segmentation is a fundamental and critical step in many image-guided clinical approaches. Recent success of deep learning-based segmentation methods usually relies on a large amount of labeled data, which is particularly difficult and costly to obtain, especially in the medical imaging domain where only experts can provide reliable and accurate annotations. Semi-supervised learning has emerged as an appealing strategy and been widely applied to medical image segmentation tasks to train deep models with limited annotations. In this paper, we present a comprehensive review of recently proposed semi-supervised learning methods for medical image segmentation and summarize both the technical novelties and empirical results. Furthermore, we analyze and discuss the limitations and several unsolved problems of existing approaches. We hope this review can inspire the research community to explore solutions to this challenge and further advance the field of medical image segmentation.},
	urldate = {2024-08-12},
	journal = {Computers in Biology and Medicine},
	author = {Jiao, Rushi and Zhang, Yichi and Ding, Le and Xue, Bingsen and Zhang, Jicong and Cai, Rong and Jin, Cheng},
	month = feb,
	year = {2024},
	keywords = {Convolutional neural network, Medical image segmentation, Semi-supervised learning, Survey},
	pages = {107840},
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	number = {6},
	urldate = {2024-08-12},
	journal = {Commun. ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2017},
	pages = {84--90},
}

@inproceedings{girshick_rich_2014,
	title = {Rich {Feature} {Hierarchies} for {Accurate} {Object} {Detection} and {Semantic} {Segmentation}},
	url = {https://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html},
	urldate = {2024-08-12},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	year = {2014},
	pages = {580--587},
}

@article{gupta_deep_2021,
	title = {Deep learning for object detection and scene perception in self-driving cars: {Survey}, challenges, and open issues},
	volume = {10},
	issn = {2590-0056},
	shorttitle = {Deep learning for object detection and scene perception in self-driving cars},
	url = {https://www.sciencedirect.com/science/article/pii/S2590005621000059},
	doi = {10.1016/j.array.2021.100057},
	abstract = {This article presents a comprehensive survey of deep learning applications for object detection and scene perception in autonomous vehicles. Unlike existing review papers, we examine the theory underlying self-driving vehicles from deep learning perspective and current implementations, followed by their critical evaluations. Deep learning is one potential solution for object detection and scene perception problems, which can enable algorithm-driven and data-driven cars. In this article, we aim to bridge the gap between deep learning and self-driving cars through a comprehensive survey. We begin with an introduction to self-driving cars, deep learning, and computer vision followed by an overview of artificial general intelligence. Then, we classify existing powerful deep learning libraries and their role and significance in the growth of deep learning. Finally, we discuss several techniques that address the image perception issues in real-time driving, and critically evaluate recent implementations and tests conducted on self-driving cars. The findings and practices at various stages are summarized to correlate prevalent and futuristic techniques, and the applicability, scalability and feasibility of deep learning to self-driving cars for achieving safe driving without human intervention. Based on the current survey, several recommendations for further research are discussed at the end of this article.},
	urldate = {2024-08-12},
	journal = {Array},
	author = {Gupta, Abhishek and Anpalagan, Alagan and Guan, Ling and Khwaja, Ahmed Shaharyar},
	month = jul,
	year = {2021},
	keywords = {Autonomous driving initiatives, Computer vision, Convolutional neural networks, Deep learning, Levels of automation, LiDAR, Machine learning, Multimodal sensor fusion, Object detection, Scene perception, Self-driving cars},
	pages = {100057},
}

@article{wang_improvement_2023,
	title = {Improvement of {Retinal} {Vessel} {Segmentation} {Method} {Based} on {U}-{Net}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/12/2/262},
	doi = {10.3390/electronics12020262},
	abstract = {Retinal vessel segmentation remains a challenging task because the morphology of the retinal vessels reflects the health of a person, which is essential for clinical diagnosis. Therefore, achieving accurate segmentation of the retinal vessel shape can determine the patient’s physical condition in a timely manner and can prevent blindness in patients. Since the traditional retinal vascular segmentation method is manually operated, this can be time-consuming and laborious. With the development of convolutional neural networks, U-shaped networks (U-Nets) and variants show good performance in image segmentation. However, U-Net is prone to feature loss due to the operation of the encoder convolution layer and also causes the problem of mismatch in the processing of contextual information features caused by the skip connection part. Therefore, we propose an improvement of the retinal vessel segmentation method based on U-Net to segment retinal vessels accurately. In order to extract more features from encoder features, we replace the convolutional layer with ResNest network structure in feature extraction, which aims to enhance image feature extraction. In addition, a Depthwise FCA Block (DFB) module is proposed to deal with the mismatched processing of local contextual features by skip connections. Combined with the two public datasets on retinal vessel segmentation, namely DRIVE and CHASE\_DB1, and comparing our method with a larger number of networks, the experimental results confirmed the effectiveness of the proposed method. Our method is better than most segmentation networks, demonstrating the method’s significant clinical value.},
	language = {en},
	number = {2},
	urldate = {2024-08-12},
	journal = {Electronics},
	author = {Wang, Ning and Li, Kefeng and Zhang, Guangyuan and Zhu, Zhenfang and Wang, Peng},
	month = jan,
	year = {2023},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {U-Net, feature extraction, retinal vessels segmentation},
	pages = {262},
}

@article{buslaev_albumentations_2020,
	title = {Albumentations: {Fast} and {Flexible} {Image} {Augmentations}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	shorttitle = {Albumentations},
	url = {https://www.mdpi.com/2078-2489/11/2/125},
	doi = {10.3390/info11020125},
	abstract = {Data augmentation is a commonly used technique for increasing both the size and the diversity of labeled training sets by leveraging input transformations that preserve corresponding output labels. In computer vision, image augmentations have become a common implicit regularization technique to combat overfitting in deep learning models and are ubiquitously used to improve performance. While most deep learning frameworks implement basic image transformations, the list is typically limited to some variations of flipping, rotating, scaling, and cropping. Moreover, image processing speed varies in existing image augmentation libraries. We present Albumentations, a fast and flexible open source library for image augmentation with many various image transform operations available that is also an easy-to-use wrapper around other augmentation libraries. We discuss the design principles that drove the implementation of Albumentations and give an overview of the key features and distinct capabilities. Finally, we provide examples of image augmentations for different computer vision tasks and demonstrate that Albumentations is faster than other commonly used image augmentation tools on most image transform operations.},
	language = {en},
	number = {2},
	urldate = {2024-08-07},
	journal = {Information},
	author = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
	month = feb,
	year = {2020},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {computer vision, data augmentation, deep learning},
	pages = {125},
}

@article{mendel_error-correcting_2023,
	title = {Error-{Correcting} {Mean}-{Teacher}: {Corrections} instead of consistency-targets applied to semi-supervised medical image segmentation},
	volume = {154},
	issn = {0010-4825},
	shorttitle = {Error-{Correcting} {Mean}-{Teacher}},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482523000501},
	doi = {10.1016/j.compbiomed.2023.106585},
	abstract = {Semantic segmentation is an essential task in medical imaging research. Many powerful deep-learning-based approaches can be employed for this problem, but they are dependent on the availability of an expansive labeled dataset. In this work, we augment such supervised segmentation models to be suitable for learning from unlabeled data. Our semi-supervised approach, termed Error-Correcting Mean-Teacher, uses an exponential moving average model like the original Mean Teacher but introduces our new paradigm of error correction. The original segmentation network is augmented to handle this secondary correction task. Both tasks build upon the core feature extraction layers of the model. For the correction task, features detected in the input image are fused with features detected in the predicted segmentation and further processed with task-specific decoder layers. The combination of image and segmentation features allows the model to correct present mistakes in the given input pair. The correction task is trained jointly on the labeled data. On unlabeled data, the exponential moving average of the original network corrects the student’s prediction. The combined outputs of the students’ prediction with the teachers’ correction form the basis for the semi-supervised update. We evaluate our method with the 2017 and 2018 Robotic Scene Segmentation data, the ISIC 2017 and the BraTS 2020 Challenges, a proprietary Endoscopic Submucosal Dissection dataset, Cityscapes, and Pascal VOC 2012. Additionally, we analyze the impact of the individual components and examine the behavior when the amount of labeled data varies, with experiments performed on two distinct segmentation architectures. Our method shows improvements in terms of the mean Intersection over Union over the supervised baseline and competing methods. Code is available at https://github.com/CloneRob/ECMT.},
	urldate = {2024-08-07},
	journal = {Computers in Biology and Medicine},
	author = {Mendel, Robert and Rauber, David and de Souza, Luis A. and Papa, João P. and Palm, Christoph},
	month = mar,
	year = {2023},
	keywords = {Mean-Teacher, Medical imaging, Pseudo-labels, Segmentation, Semi-supervised},
	pages = {106585},
}

@inproceedings{chang_semantic_2020,
	address = {Wiesbaden},
	title = {Semantic {Lung} {Segmentation} {Using} {Convolutional} {Neural} {Networks}},
	isbn = {978-3-658-29267-6},
	doi = {10.1007/978-3-658-29267-6_17},
	abstract = {Chest X-Ray (CXR) images as part of a non-invasive diagnosis method are commonly used in today’s medical workflow. In traditional methods, physicians usually use their experience to interpret CXR images, however, there is a large interobserver variance. Computer vision may be used as a standard for assisted diagnosis. In this study, we applied an encoder-decoder neural network architecture for automatic lung region detection. We compared a three-class approach (left lung, right lung, background) and a two-class approach (lung, background). The differentiation of left and right lungs as direct result of a semantic segmentation on basis of neural nets rather than post-processing a lung-background segmentation is done here for the first time. Our evaluation was done on the NIH Chest X-ray dataset, from which 1736 images were extracted and manually annotated. We achieved 94:9\% mIoU and 92\% mIoU as segmentation quality measures for the two-class-model and the three-class-model, respectively. This result is very promising for the segmentation of lung regions having the simultaneous classification of left and right lung in mind.},
	language = {de},
	booktitle = {Bildverarbeitung für die {Medizin} 2020},
	publisher = {Springer Fachmedien},
	author = {Chang, Ching-Sheng and Lin, Jin-Fa and Lee, Ming-Ching and Palm, Christoph},
	editor = {Tolxdorff, Thomas and Deserno, Thomas M. and Handels, Heinz and Maier, Andreas and Maier-Hein, Klaus H. and Palm, Christoph},
	year = {2020},
	pages = {75--80},
}

@misc{noauthor_source1_nodate,
	title = {{SOURCE1}},
	url = {http://vital.seals.ac.za:8080/vital/access/manager/PdfViewer/vital:71937/SOURCE1?viewPdfInternal=1},
	urldate = {2024-08-07},
}

@article{czimmermann_visual-based_2020,
	title = {Visual-{Based} {Defect} {Detection} and {Classification} {Approaches} for {Industrial} {Applications}—{A} {SURVEY}},
	volume = {20},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/5/1459},
	doi = {10.3390/s20051459},
	abstract = {This paper reviews automated visual-based defect detection approaches applicable to various materials, such as metals, ceramics and textiles. In the first part of the paper, we present a general taxonomy of the different defects that fall in two classes: visible (e.g., scratches, shape error, etc.) and palpable (e.g., crack, bump, etc.) defects. Then, we describe artificial visual processing techniques that are aimed at understanding of the captured scenery in a mathematical/logical way. We continue with a survey of textural defect detection based on statistical, structural and other approaches. Finally, we report the state of the art for approaching the detection and classification of defects through supervised and non-supervised classifiers and deep learning.},
	language = {en},
	number = {5},
	urldate = {2024-08-06},
	journal = {Sensors},
	author = {Czimmermann, Tamás and Ciuti, Gastone and Milazzo, Mario and Chiurazzi, Marcello and Roccella, Stefano and Oddo, Calogero Maria and Dario, Paolo},
	month = jan,
	year = {2020},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {classification, deep learning, defect detection, industry 4.0, survey},
	pages = {1459},
}

@book{lohweg_bildverarbeitung_2023,
	address = {Berlin, Heidelberg},
	series = {Technologien für die intelligente {Automation}},
	title = {Bildverarbeitung in der {Automation}: {Ausgewählte} {Beiträge} des {Jahreskolloquiums} {BVAu} 2022},
	volume = {17},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	isbn = {978-3-662-66768-2 978-3-662-66769-9},
	shorttitle = {Bildverarbeitung in der {Automation}},
	url = {https://link.springer.com/10.1007/978-3-662-66769-9},
	language = {de},
	urldate = {2024-08-06},
	publisher = {Springer},
	editor = {Lohweg, Volker},
	year = {2023},
	doi = {10.1007/978-3-662-66769-9},
	keywords = {Conference Proceedings, Data Augmentation, Deep Learning, Industrielle Bildverarbeitung, Open Access, Optische Anomalieerkennung, Optische Qualitätsinspektion},
}

@inproceedings{leyendecker_study_2023,
	address = {Berlin, Heidelberg},
	title = {A {Study} on {Data} {Augmentation} {Techniques} for {Visual} {Defect} {Detection} in {Manufacturing}},
	isbn = {978-3-662-66769-9},
	doi = {10.1007/978-3-662-66769-9_6},
	abstract = {Deep learning-based defect detection is rapidly gaining importance for automating visual quality control tasks in industrial applications. However, due to usually low rejection rates in manufacturing processes, industrial defect detection datasets are inherent to three severe data challenges: data sparsity, data imbalance, and data shift. Because the acquisition of defect data is highly cost″​=intensive, and Deep Learning (DL) algorithms require a sufficiently large amount of data, we are investigating how to solve these challenges using data oversampling and data augmentation (DA) techniques. Given the problem of binary defect detection, we present a novel experimental procedure for analyzing the impact of different DA-techniques. Accordingly, pre-selected DA-techniques are used to generate experiments across multiple datasets and DL models. For each defect detection use-case, we configure a set of random DA-pipelines to generate datasets of different characteristics. To investigate the impact of DA-techniques on defect detection performance, we then train convolutional neural networks with two different but fixed architectures and hyperparameter sets. To quantify and evaluate the generalizability, we compute the distances between dataset derivatives to determine the degree of domain shift. The results show that we can precisely analyze the influences of individual DA-methods, thus laying the foundation for establishing a mapping between dataset properties and DA-induced performance enhancement aiming for enhancing DL development. We show that there is no one-fits all solution, but that within the categories of geometrical and color augmentations, certain DA-methods outperform others.},
	language = {de},
	booktitle = {Bildverarbeitung in der {Automation}},
	publisher = {Springer},
	author = {Leyendecker, Lars and Agarwal, Shobhit and Werner, Thorben and Motz, Maximilian and Schmitt, Robert H.},
	editor = {Lohweg, Volker},
	year = {2023},
	keywords = {Data augmentation, Deep learning, Defect detection, Manufacturing, Visual quality control},
	pages = {73--94},
}

@article{khaledyan_enhancing_2023,
	title = {Enhancing breast ultrasound segmentation through fine-tuning and optimization techniques: {Sharp} attention {UNet}},
	volume = {18},
	issn = {1932-6203},
	shorttitle = {Enhancing breast ultrasound segmentation through fine-tuning and optimization techniques},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0289195},
	doi = {10.1371/journal.pone.0289195},
	abstract = {Segmentation of breast ultrasound images is a crucial and challenging task in computer-aided diagnosis systems. Accurately segmenting masses in benign and malignant cases and identifying regions with no mass is a primary objective in breast ultrasound image segmentation. Deep learning (DL) has emerged as a powerful tool in medical image segmentation, revolutionizing how medical professionals analyze and interpret complex imaging data. The UNet architecture is a highly regarded and widely used DL model in medical image segmentation. Its distinctive architectural design and exceptional performance have made it popular among researchers. With the increase in data and model complexity, optimization and fine-tuning models play a vital and more challenging role than before. This paper presents a comparative study evaluating the effect of image preprocessing and different optimization techniques and the importance of fine-tuning different UNet segmentation models for breast ultrasound images. Optimization and fine-tuning techniques have been applied to enhance the performance of UNet, Sharp UNet, and Attention UNet. Building upon this progress, we designed a novel approach by combining Sharp UNet and Attention UNet, known as Sharp Attention UNet. Our analysis yielded the following quantitative evaluation metrics for the Sharp Attention UNet: the Dice coefficient, specificity, sensitivity, and F1 score values obtained were 0.93, 0.99, 0.94, and 0.94, respectively. In addition, McNemar’s statistical test was applied to assess significant differences between the approaches. Across a number of measures, our proposed model outperformed all other models, resulting in improved breast lesion segmentation.},
	language = {en},
	number = {12},
	urldate = {2024-07-31},
	journal = {PLOS ONE},
	author = {Khaledyan, Donya and Marini, Thomas J. and Baran, Timothy M. and O’Connell, Avice and Parker, Kevin},
	month = dec,
	year = {2023},
	note = {Publisher: Public Library of Science},
	keywords = {Breast cancer, Imaging techniques, Neural networks, Neurons, Optimization, Preprocessing, Signal decoders, Ultrasound imaging},
	pages = {e0289195},
}

@article{jumaboev_photovoltaics_2022,
	title = {Photovoltaics {Plant} {Fault} {Detection} {Using} {Deep} {Learning} {Techniques}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/15/3728},
	doi = {10.3390/rs14153728},
	abstract = {Solar energy is the fastest-growing clean and sustainable energy source, outperforming other forms of energy generation. Usually, solar panels are low maintenance and do not require permanent service. However, plenty of problems can result in a production loss of up to {\textasciitilde}20\% since a failed panel will impact the generation of a whole array. High-quality and timely maintenance of the power plant will reduce the cost of its repair and, most importantly, increase the life of the power plant and the total generation of electricity. Manual monitoring of panels is costly and time-consuming on large solar plantations; moreover, solar plantations located distantly are more complicated for humans to access. This paper presents deep learning-based photovoltaics fault detection techniques using thermal images obtained from an unmanned aerial vehicle (UAV) equipped with infrared sensors. We implemented the three most accurate segmentation models to detect defective panels on large solar plantations. The models employed in this work are DeepLabV3+, Feature Pyramid Network (FPN) and U-Net with different encoder architectures. The obtained results revealed intersection over union (IoU) of 79\%, 85\%, 86\%, and dice coefficients of 87\%, 92\%, 94\% for DeepLabV3+, FPN, and U-Net, respectively. The implemented models showed efficient performance and proved effective to resolve these challenges.},
	language = {en},
	number = {15},
	urldate = {2024-07-29},
	journal = {Remote Sensing},
	author = {Jumaboev, Sherozbek and Jurakuziev, Dadajon and Lee, Malrey},
	month = jan,
	year = {2022},
	note = {Number: 15
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {DeepLabV3+, FPN, PV plant, U-Net, fault detection, panel defects, semantic segmentation},
	pages = {3728},
}

@article{schnitzler_automatic_2024,
	title = {Automatic {Detection} of {Post}-{Operative} {Clips} in {Mammography} {Using} a {U}-{Net} {Convolutional} {Neural} {Network}},
	volume = {10},
	issn = {2313-433X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11205252/},
	doi = {10.3390/jimaging10060147},
	abstract = {Background: After breast conserving surgery (BCS), surgical clips indicate the tumor bed and, thereby, the most probable area for tumor relapse. The aim of this study was to investigate whether a U-Net-based deep convolutional neural network (dCNN) may be used to detect surgical clips in follow-up mammograms after BCS. Methods: 884 mammograms and 517 tomosynthetic images depicting surgical clips and calcifications were manually segmented and classified. A U-Net-based segmentation network was trained with 922 images and validated with 394 images. An external test dataset consisting of 39 images was annotated by two radiologists with up to 7 years of experience in breast imaging. The network’s performance was compared to that of human readers using accuracy and interrater agreement (Cohen’s Kappa). Results: The overall classification accuracy on the validation set after 45 epochs ranged between 88.2\% and 92.6\%, indicating that the model’s performance is comparable to the decisions of a human reader. In 17.4\% of cases, calcifications have been misclassified as post-operative clips. The interrater reliability of the model compared to the radiologists showed substantial agreement (κreader1 = 0.72, κreader2 = 0.78) while the readers compared to each other revealed a Cohen’s Kappa of 0.84, thus showing near-perfect agreement. Conclusions: With this study, we show that surgery clips can adequately be identified by an AI technique. A potential application of the proposed technique is patient triage as well as the automatic exclusion of post-operative cases from PGMI (Perfect, Good, Moderate, Inadequate) evaluation, thus improving the quality management workflow.},
	number = {6},
	urldate = {2024-07-14},
	journal = {Journal of Imaging},
	author = {Schnitzler, Tician and Ruppert, Carlotta and Hejduk, Patryk and Borkowski, Karol and Kajüter, Jonas and Rossi, Cristina and Ciritsis, Alexander and Landsmann, Anna and Zaytoun, Hasan and Boss, Andreas and Schindera, Sebastian and Burn, Felice},
	month = jun,
	year = {2024},
	pmid = {38921624},
	pmcid = {PMC11205252},
	pages = {147},
}

@article{saidu_active_2021,
	title = {Active {Learning} with {Bayesian} {UNet} for {Efficient} {Semantic} {Image} {Segmentation}},
	volume = {7},
	issn = {2313-433X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8321278/},
	doi = {10.3390/jimaging7020037},
	abstract = {We present a sample-efficient image segmentation method using active learning, we call it Active Bayesian UNet, or AB-UNet. This is a convolutional neural network using batch normalization and max-pool dropout. The Bayesian setup is achieved by exploiting the probabilistic extension of the dropout mechanism, leading to the possibility to use the uncertainty inherently present in the system. We set up our experiments on various medical image datasets and highlight that with a smaller annotation effort our AB-UNet leads to stable training and better generalization. Added to this, we can efficiently choose from an unlabelled dataset.},
	number = {2},
	urldate = {2024-07-14},
	journal = {Journal of Imaging},
	author = {Saidu, Isah Charles and Csató, Lehel},
	month = feb,
	year = {2021},
	pmid = {34460636},
	pmcid = {PMC8321278},
	pages = {37},
}

@article{tauer_transparente_2022,
	title = {{TRANSPARENTE} {LIEFERKETTE} {DANK} Ü{BERGREIFENDER} {IT}-{LÖSUNG}},
	language = {de},
	author = {Tauer, Rebecca},
	year = {2022},
}

@inproceedings{ciresan_deep_2012,
	title = {Deep {Neural} {Networks} {Segment} {Neuronal} {Membranes} in {Electron} {Microscopy} {Images}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper/2012/hash/459a4ddcb586f24efd9395aa7662bc7c-Abstract.html},
	urldate = {2024-06-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ciresan, Dan and Giusti, Alessandro and Gambardella, Luca and Schmidhuber, Jürgen},
	year = {2012},
}

@misc{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more eﬃciently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caﬀe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {en},
	urldate = {2024-06-10},
	publisher = {arXiv},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv:1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{wu_general_2022,
	title = {A general deep learning framework for neuron instance segmentation based on {Efficient} {UNet} and morphological post-processing},
	volume = {150},
	issn = {0010-4825},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482522008885},
	doi = {10.1016/j.compbiomed.2022.106180},
	abstract = {Recent studies have demonstrated the superiority of deep learning in medical image analysis, especially in cell instance segmentation, a fundamental step for many biological studies. However, the excellent performance of the neural networks requires training on large, unbiased dataset and annotations, which is labor-intensive and expertise-demanding. This paper presents an end-to-end framework to automatically detect and segment NeuN stained neuronal cells on histological images using only point annotations. Unlike traditional nuclei segmentation with point annotation, we propose using point annotation and binary segmentation to synthesize pixel-level annotations. The synthetic masks are used as the ground truth to train the neural network, a U-Net-like architecture with a state-of-the-art network, EfficientNet, as the encoder. Validation results show the superiority of our model compared to other recent methods. In addition, we investigated multiple post-processing schemes and proposed an original strategy to convert the probability map into segmented instances using ultimate erosion and dynamic reconstruction. This approach is easy to configure and outperforms other classical post-processing techniques. This work aims to develop a robust and efficient framework for analyzing neurons using optical microscopic data, which can be used in preclinical biological studies and, more specifically, in the context of neurodegenerative diseases. Code is available at: https://github.com/MIRCen/NeuronInstanceSeg.},
	urldate = {2024-06-10},
	journal = {Computers in Biology and Medicine},
	author = {Wu, Huaqian and Souedet, Nicolas and Jan, Caroline and Clouchoux, Cédric and Delzescaux, Thierry},
	month = nov,
	year = {2022},
	keywords = {Deep learning, Histological images, Mathematical morphology, Neuron instance segmentation, Optical microscopy},
	pages = {106180},
}

@article{forsberg_detection_2017,
	title = {Detection and {Labeling} of {Vertebrae} in {MR} {Images} {Using} {Deep} {Learning} with {Clinical} {Annotations} as {Training} {Data}},
	volume = {30},
	issn = {1618-727X},
	url = {https://doi.org/10.1007/s10278-017-9945-x},
	doi = {10.1007/s10278-017-9945-x},
	abstract = {The purpose of this study was to investigate the potential of using clinically provided spine label annotations stored in a single institution image archive as training data for deep learning-based vertebral detection and labeling pipelines. Lumbar and cervical magnetic resonance imaging cases with annotated spine labels were identified and exported from an image archive. Two separate pipelines were configured and trained for lumbar and cervical cases respectively, using the same setup with convolutional neural networks for detection and parts-based graphical models to label the vertebrae. The detection sensitivity, precision and accuracy rates ranged between 99.1–99.8, 99.6–100, and 98.8–99.8\% respectively, the average localization error ranges were 1.18–1.24 and 2.38–2.60 mm for cervical and lumbar cases respectively, and with a labeling accuracy of 96.0–97.0\%. Failed labeling results typically involved failed S1 detections or missed vertebrae that were not fully visible on the image. These results show that clinically annotated image data from one image archive is sufficient to train a deep learning-based pipeline for accurate detection and labeling of MR images depicting the spine. Further, these results support using deep learning to assist radiologists in their work by providing highly accurate labels that only require rapid confirmation.},
	language = {en},
	number = {4},
	urldate = {2024-06-07},
	journal = {Journal of Digital Imaging},
	author = {Forsberg, Daniel and Sjöblom, Erik and Sunshine, Jeffrey L.},
	month = aug,
	year = {2017},
	keywords = {Archive, Artificial neural networks (ANNs), Machine learning, Magnetic resonance imaging},
	pages = {406--412},
}

@article{kubina_expose_2020,
	title = {Exposé – {Bibliotheken} in der {Transformation} zu {Smart} {Libraries} : {Wie} verändert sich die {Bibliotheksarbeit} durch den {Einsatz} von {Internet}-of-{Things}-{Technologien}?},
	volume = {1},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2699-6693},
	shorttitle = {Exposé – {Bibliotheken} in der {Transformation} zu {Smart} {Libraries}},
	url = {https://reposit.haw-hamburg.de/handle/20.500.12738/11146},
	doi = {10.15460/apimagazin.2020.1.17},
	abstract = {Im vorliegenden Beitrag wird exemplarisch die Ausarbeitung eines Exposés für die Abschlussarbeit  im  BA-Studiengang  Bibliotheks-  und  Informationsmanagement (BIM) vorgestellt. Anhand des Themas „Smart Libraries“ werden die typischen Be-standteile eines Exposés veranschaulicht. Es kann damit als  Anregung für alle Stu-dierenden dienen, die in der Abschlussphase ihres Studiums vor dieser Aufgabenstel-lung stehen.},
	language = {de},
	number = {1},
	urldate = {2024-06-07},
	journal = {Ausbilden, Publizieren, Informieren : API : studentisches Magazin der HAW Hamburg},
	author = {Kubina, Michael},
	month = jan,
	year = {2020},
	note = {Accepted: 2021-06-17T18:59:30Z
Publisher: Hamburg University Press},
}

@article{tripathi_analysis_2021,
	title = {Analysis of {Convolutional} {Neural} {Network} based {Image} {Classification} {Techniques}},
	volume = {3},
	doi = {10.36548/jiip.2021.2.003},
	abstract = {With the rapid urbanization and people moving from rural areas to urban time has become a very huge commodity. As a result of this change in people's lifestyles, there is a growing need for speed and efficiency. In the supermarket industry, item identification and billing are generally done manually, which takes a lot of time and effort. The lack of a bar code on the fruit products slows down the processing time. Before beginning the billing process, the seller may need to weigh the items in order to update the barcode, or the biller may need to input the item's name manually. This doubles the effort and also consumes a significant amount of time. As a result, several convolutional neural network-based classifiers are proposed to identify the fruits by visualizing via the camera for establishing a quick billing procedure in order to overcome this difficulty. The best model among the suggested models is capable of classifying pictures with start-of-art accuracy, which is superior than that of previously published studies.},
	journal = {Journal of Innovative Image Processing},
	author = {Tripathi, Milan},
	month = jun,
	year = {2021},
	pages = {100--117},
}

@phdthesis{wicht_deep_2018,
	title = {Deep {Learning} feature {Extraction} for {Image} {Processing}},
	abstract = {In this thesis, we propose to use methodologies that automatically learn how to extract relevant features from images. We are especially interested in evaluating how these features compare against handcrafted features. More precisely, we are interested in the unsupervised training that is used for the Restricted Boltzmann Machine (RBM) and Convolutional RBM (CRBM) models. These models relaunched the Deep Learning interest of the last decade. During the time of this thesis, the auto-encoders approach, especially Convolutional Auto-Encoders (CAE) have been used more and more. Therefore, one objective of this thesis is also to compare the CRBM approach with the CAE approach.

The scope of this work is defined by several machine learning tasks. The first one, handwritten digit recognition, is analysed to see how much the unsupervised pretraining technique introduced with the Deep Belief Network (DBN) model improves the training of neural networks. The second, detection and recognition of Sudoku in images, is evaluating the efficiency of DBN and Convolutional DBN (CDBN) models for classification of images of poor quality. Finally, features are learned fully unsupervised from images for a keyword spotting task and are compared against well-known handcrafted features. Moreover, the thesis was also oriented around a software engineering axis. Indeed, a complete machine learning framework was developed during this thesis to explore possible optimizations and possible algorithms in order to train the tested models as fast as possible.},
	author = {Wicht, Baptiste},
	month = jan,
	year = {2018},
	doi = {10.13140/RG.2.2.11700.35207},
}

@phdthesis{mahmud_evaluation_2024,
	title = {Evaluation of {Deep} {Learning} {Models} for the {Detection} of {Skin} {Cancer} {With} {Dermatoscopic} {Images} of {Pigmented} {Lesions}},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	url = {https://www.proquest.com/docview/3057691506/BD6E27BC45F44FD5PQ/7?sourcetype=Dissertations%20&%20Theses},
	abstract = {Skin lesion classification using deep learning models has emerged as a promising approach for automated diagnosis and classification of skin diseases. In this study, we explore the effectiveness of three popular deep learning architectures - VGG16, ResNet50, and DenseNet201 - for the classification of skin lesions. We investigate the performance of these models on a dataset of skin lesion images, focusing on the impact of varying training set sizes and data augmentation techniques. Additionally, we compare the training and validation accuracies of each model across different iterations to assess their learning capabilities. Our results demonstrate the effectiveness of deep learning models in accurately classifying skin lesions, with DenseNet201 consistently outperforming VGG16 and ResNet50 in terms of classification accuracy. Furthermore, we analyze the computational costs associated with training each model, providing insights into the training and validation times across different architectures and batch sizes. Overall, this study contributes to the understanding of deep learning approaches for skin lesion classification and provides valuable insights into the performance and computational considerations of different deep learning models in this domain.},
	language = {English},
	urldate = {2024-05-27},
	school = {ProQuest Dissertation \& Theses},
	author = {Mahmud, Al},
	year = {2024},
	note = {ISBN: 9798382737249},
}

@phdthesis{chen_deep_2022,
	title = {Deep {Learning} {Applications} in {Biomedical} {Sciences} and {Bioinformatics}},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	url = {https://www.proquest.com/docview/2898818013/BD6E27BC45F44FD5PQ/1?sourcetype=Dissertations%20&%20Theses},
	abstract = {Deep learning has been applied to solve complex problems in a variety of scientific domains including biological and medical sciences. In particular, convolutional neural networks (CNNs) have displayed state-of-the-art results in computer vision tasks, outperforming humans in numerous cases. In this work, we explore several deep learning applications in medicine and bioinformatics. We show that deep learning can be applied to real-world medical procedures such as colon screening, and assist physicians with polyp detection and segmentation, and that deep learning can be applied to optical surgery, and assist physicians with image quality enhancement. In the field of bioinformatics, advances in high-throughput sequencing technologies have greatly lowered the cost of genome-wide sequencing, allowing researchers to include a larger number of experimental replicates than they previous could. This allows an easier application of deep learning as well as other statistical models to solve the problems in bioinformatics, such as detecting circadian rhythm in high-throughput omic data. To access and mine circadian datasets in a comprehensive and integrated way, we curate over 227 high-thoughput circadian datasets across different species and tissues, apply a deep learning model named BIO CYCLE together with other statistical methods to detect circadian pattern in the omic data, and build a web portal http://circadiomics.ics.uci.edu for search and visualization.},
	language = {English},
	urldate = {2024-05-27},
	school = {ProQuest Dissertation \& Theses},
	author = {Chen, Siwei},
	year = {2022},
	note = {ISBN: 9798381106886},
}

@inproceedings{maheswari_survey_2024,
	address = {Bengaluru, India},
	title = {A {Survey} on {Detection} of {Various} {Casting} {Defects} {Using} {Deep} {Learning} {Techniques}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350327533},
	url = {https://ieeexplore.ieee.org/document/10467829/},
	doi = {10.1109/IDCIoT59759.2024.10467829},
	abstract = {In the mass production process, producing the product quality is a challenging task, because of the metal casting process, the presence of the product varies irregularly due to the kinds of defects. Identification of faulty products early in an automatic manner is one of the challenges in the industry. This work is a systematic review of various kinds of defects in the casting process, automated defect detection systems with deep learning approaches, and an analysis of their performance. Deep learning approaches are used to produce high-quality products in production lines and enhance the quality inspection process earlier in an automatic manner.},
	language = {en},
	urldate = {2024-05-23},
	booktitle = {2024 2nd {International} {Conference} on {Intelligent} {Data} {Communication} {Technologies} and {Internet} of {Things} ({IDCIoT})},
	publisher = {IEEE},
	author = {Maheswari, M. and Brintha, N. C.},
	month = jan,
	year = {2024},
	pages = {1436--1440},
}

@misc{noauthor_plagiatsverdacht_nodate,
	title = {Plagiatsverdacht: {Von} {Guttenberg} bis {Giffey} - {Bildung} - {SZ}.de},
	url = {https://www.sueddeutsche.de/bildung/giffey-guttenberg-plagiat-doktorarbeit-1.1592095},
	urldate = {2024-04-24},
}
